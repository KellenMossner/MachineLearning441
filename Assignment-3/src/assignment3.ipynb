{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5727a8",
   "metadata": {},
   "source": [
    "# Assignment 3: Active Learning in Neural Networks\n",
    "## Machine Learning 441\n",
    "Kellen Mossner\n",
    "26024284@sun.ac.za"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f007089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold \n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "import tqdm\n",
    "from itertools import product\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from NeuralNetwork import NeuralNetwork\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# IEEE dual-column specifications\n",
    "column_width_pt = 241.14734  # Exact IEEE dual-column width\n",
    "text_width_pt = 516.0        # Full text width (both columns + gap)\n",
    "\n",
    "column_width_inches = column_width_pt / 72.27\n",
    "text_width_inches = text_width_pt / 72.27\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.size\": 10,           # Base font size to match document\n",
    "    \"axes.labelsize\": 10,      # x and y axis labels\n",
    "    \"axes.titlesize\": 10,      # Plot title\n",
    "    \"xtick.labelsize\": 10,      # x-axis tick labels (slightly smaller)\n",
    "    \"ytick.labelsize\": 10,      # y-axis tick labels (slightly smaller)\n",
    "    \"legend.fontsize\": 10,      # Legend text\n",
    "    \"figure.titlesize\": 10,    # Figure title (if used)\n",
    "    \n",
    "    # Line and marker properties\n",
    "    \"lines.linewidth\": 1.2,\n",
    "    \"lines.markersize\": 4,\n",
    "    \"patch.linewidth\": 0.5,\n",
    "    \n",
    "    # Grid properties\n",
    "    \"grid.linewidth\": 0.5,\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \n",
    "    # Save properties\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"savefig.bbox\": \"tight\",\n",
    "    \"savefig.format\": \"pdf\",\n",
    "    \"savefig.pad_inches\": 0.02,\n",
    "})\n",
    "\n",
    "import scikit_posthocs as sp\n",
    "from scipy.stats import rankdata, wilcoxon\n",
    "from scipy.stats import friedmanchisquare\n",
    "from scipy.stats import rankdata\n",
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af697c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality reports\n",
    "def quality_report(df):\n",
    "    \"\"\"\n",
    "    Generate a quality report for the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    report_cts = df.describe(include=[np.number]).T\n",
    "    report_cts[\"Cardinality\"] = df.nunique()\n",
    "    report_cts[\"Missing Percentage\"] = (df.isnull().sum() / len(df)) * 100\n",
    "    report_cts.rename(\n",
    "        columns={\n",
    "            \"50%\": \"Median\",\n",
    "            \"25%\": \"1st Qrt\",\n",
    "            \"75%\": \"3rd Qrt\",\n",
    "            \"mean\": \"Mean\",\n",
    "            \"count\": \"Count\",\n",
    "            \"max\": \"Max\",\n",
    "            \"std\": \"Std Dev\",\n",
    "            \"min\": \"Min\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    report_cts[\"Count\"] = len(df)\n",
    "    report_cts.reset_index(inplace=True)\n",
    "    report_cts.rename(columns={\"index\": \"Feature\"}, inplace=True)\n",
    "\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    cat_data = []\n",
    "    for col in categorical_cols:\n",
    "        value_counts = df[col].value_counts()\n",
    "        total_count = len(df[col])\n",
    "        mode = value_counts.index[0] if len(value_counts) > 0 else None\n",
    "        mode_freq = value_counts.iloc[0] if len(value_counts) > 0 else 0\n",
    "        mode_pct = (mode_freq / total_count * 100) if total_count > 0 else 0\n",
    "        second_mode = value_counts.index[1] if len(value_counts) > 1 else None\n",
    "        second_mode_freq = value_counts.iloc[1] if len(value_counts) > 1 else 0\n",
    "        second_mode_pct = (\n",
    "            (second_mode_freq / total_count * 100) if total_count > 0 else 0\n",
    "        )\n",
    "        cat_data.append(\n",
    "            {\n",
    "                \"Feature\": col,\n",
    "                \"Count\": total_count,\n",
    "                \"Missing Percentage\": round(\n",
    "                    (df[col].isnull().sum() / len(df)) * 100, 2\n",
    "                ),\n",
    "                \"Unique\": df[col].nunique(),\n",
    "                \"Mode\": mode,\n",
    "                \"Mode Freq\": mode_freq,\n",
    "                \"Mode %\": round(mode_pct, 2),\n",
    "                \"2nd Mode\": second_mode,\n",
    "                \"2nd Mode Freq\": second_mode_freq,\n",
    "                \"2nd Mode %\": round(second_mode_pct, 2),\n",
    "                \"Cardinality\": df[col].nunique(),\n",
    "            }\n",
    "        )\n",
    "    report_cat = pd.DataFrame(cat_data)\n",
    "\n",
    "    # return both data quality reports\n",
    "    return report_cts, report_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1f94b",
   "metadata": {},
   "source": [
    "### Data Pre-processing: Classification Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bdd2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Breast Cancer Dataset\n",
    "classification1 = pd.read_csv('../data/breast_cancer.csv')\n",
    "classification1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42471a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification1.drop(columns=['Unnamed: 32'], inplace=True)  # drop empty column\n",
    "X = classification1.drop(columns=['diagnosis', 'id'])\n",
    "y = classification1['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "# No data quality issues\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Class distribution: {y.value_counts()}\")\n",
    "print(f\"Missing values: {X.isnull().sum().sum()}\")\n",
    "\n",
    "# Create 70-10-20 train-val-test split\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train1, X_val1, y_train1, y_val1 = train_test_split(X_train1, y_train1, test_size=0.125, random_state=42, stratify=y_train1)\n",
    "\n",
    "# Create 1 hot encoding for categorical response vectors\n",
    "y_train1 = pd.get_dummies(y_train1).values\n",
    "y_val1 = pd.get_dummies(y_val1).values\n",
    "y_test1 = pd.get_dummies(y_test1).values\n",
    "print(f\"Response shape: {y_train1.shape}, {y_val1.shape}, {y_test1.shape}\")\n",
    "\n",
    "# Scale features\n",
    "scaler1 = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train1 = scaler1.fit_transform(X_train1)\n",
    "X_val1 = scaler1.transform(X_val1)\n",
    "X_test1 = scaler1.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c5463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Mobile Price Dataset\n",
    "classification2 = pd.read_csv('../data/mobile.csv')\n",
    "classification2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aee57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_cts, report_cat = quality_report(classification2)\n",
    "report_cts.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = classification2.drop(columns=['price_range'])\n",
    "y = classification2['price_range']\n",
    "# No data quality issues\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Class distribution: {y.value_counts()}\")\n",
    "print(f\"Missing values: {X.isnull().sum().sum()}\")\n",
    "\n",
    "# Create 70-10-20 train-val-test split\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(X_train2, y_train2, test_size=0.125, random_state=42, stratify=y_train2)\n",
    "\n",
    "# Create 1 hot encoding for categorical response vectors\n",
    "y_train2 = pd.get_dummies(y_train2).values\n",
    "y_val2 = pd.get_dummies(y_val2).values\n",
    "y_test2 = pd.get_dummies(y_test2).values\n",
    "print(f\"Response shape: {y_train2.shape}, {y_val2.shape}, {y_test2.shape}\")\n",
    "\n",
    "# Scale features\n",
    "scaler2 = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train2 = scaler2.fit_transform(X_train2)\n",
    "X_val2 = scaler2.transform(X_val2)\n",
    "X_test2 = scaler2.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Letter Recognition Dataset\n",
    "classification3 = pd.read_csv('../data/letter_recognition.csv')\n",
    "classification3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534f0a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_cts, report_cat = quality_report(classification3)\n",
    "report_cts.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b54465",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_cat.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46601741",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "classification3['lettr'] = label_encoder.fit_transform(classification3['lettr'])\n",
    "\n",
    "X = classification3.drop(columns=['lettr'])\n",
    "y = classification3['lettr']\n",
    "\n",
    "# No data quality issues\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Class distribution: {y.value_counts()}\")\n",
    "print(f\"Missing values: {X.isnull().sum().sum()}\")\n",
    "\n",
    "# Create 70-10-20 train-val-test split\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train3, X_val3, y_train3, y_val3 = train_test_split(X_train3, y_train3, test_size=0.125, random_state=42, stratify=y_train3)\n",
    "\n",
    "# Create 1 hot encoding for categorical response vectors\n",
    "y_train3 = pd.get_dummies(y_train3).values\n",
    "y_val3 = pd.get_dummies(y_val3).values\n",
    "y_test3 = pd.get_dummies(y_test3).values\n",
    "print(f\"Response shape: {y_train3.shape}, {y_val3.shape}, {y_test3.shape}\")\n",
    "\n",
    "# Scale features\n",
    "scaler3 = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train3 = scaler3.fit_transform(X_train3)\n",
    "X_val3 = scaler3.transform(X_val3)\n",
    "X_test3 = scaler3.transform(X_test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af8b007",
   "metadata": {},
   "source": [
    "### Data Pre-processing: Regression Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d9af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Boston Housing Dataset\n",
    "regression1 = pd.read_csv('../data/housing.csv')\n",
    "regression1.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0933e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_cts, report_cat = quality_report(regression1)\n",
    "report_cts.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c748beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a8c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = regression1.drop(columns=['MEDV'])\n",
    "y = regression1['MEDV']\n",
    "\n",
    "# Impute missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Missing values: {np.isnan(X).sum()}\")\n",
    "\n",
    "# Create 70-10-20 train-val-test split\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train4, X_val4, y_train4, y_val4 = train_test_split(X_train4, y_train4, test_size=0.125, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler4 = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train4 = scaler4.fit_transform(X_train4)\n",
    "X_val4 = scaler4.transform(X_val4)\n",
    "X_test4 = scaler4.transform(X_test4)\n",
    "\n",
    "# Scale target variable to output range of sigmoid activation function\n",
    "y_scaler4 = MinMaxScaler(feature_range=(0, 1))\n",
    "y_train4 = y_scaler4.fit_transform(y_train4.values.reshape(-1, 1))\n",
    "y_val4 = y_scaler4.transform(y_val4.values.reshape(-1, 1))\n",
    "y_test4 = y_scaler4.transform(y_test4.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a496c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Concrete Compressive Strength Dataset\n",
    "regression2 = pd.read_csv('../data/concrete.csv')\n",
    "regression2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e076c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_cts, report_cat = quality_report(regression2)\n",
    "report_cts.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00144849",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e65ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = regression2.drop(columns=['Concrete compressive strength'])\n",
    "y = regression2['Concrete compressive strength']\n",
    "\n",
    "# No data quality issues\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Missing values: {X.isnull().sum().sum()}\")\n",
    "\n",
    "# Create 70-10-20 train-val-test split\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train5, X_val5, y_train5, y_val5 = train_test_split(X_train5, y_train5, test_size=0.125, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler5 = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train5 = scaler5.fit_transform(X_train5)\n",
    "X_val5 = scaler5.transform(X_val5)\n",
    "X_test5 = scaler5.transform(X_test5)\n",
    "\n",
    "# Scale target variable to output range of sigmoid activation function\n",
    "y_scaler5 = MinMaxScaler(feature_range=(0, 1))\n",
    "y_train5 = y_scaler5.fit_transform(y_train5.values.reshape(-1, 1))\n",
    "y_val5 = y_scaler5.transform(y_val5.values.reshape(-1, 1))\n",
    "y_test5 = y_scaler5.transform(y_test5.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e790ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Abalone Dataset\n",
    "regression3 = pd.read_csv('../data/abalone.csv')\n",
    "regression3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fdf3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_cts, report_cat = quality_report(regression3)\n",
    "report_cts.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a302bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beba3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for categorical feature\n",
    "regression3 = pd.get_dummies(regression3, columns=['Sex'], drop_first=True)\n",
    "X = regression3.drop(columns=['Rings'])\n",
    "y = regression3['Rings']\n",
    "\n",
    "# No data quality issues\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Missing values: {X.isnull().sum().sum()}\")\n",
    "\n",
    "# Create 70-10-20 train-val-test split\n",
    "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train6, X_val6, y_train6, y_val6 = train_test_split(X_train6, y_train6, test_size=0.125, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler6 = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train6 = scaler6.fit_transform(X_train6)\n",
    "X_val6 = scaler6.transform(X_val6)\n",
    "X_test6 = scaler6.transform(X_test6)\n",
    "\n",
    "# Scale target variable to output range of sigmoid activation function\n",
    "y_scaler6 = MinMaxScaler(feature_range=(0, 1))\n",
    "y_train6 = y_scaler6.fit_transform(y_train6.values.reshape(-1, 1))\n",
    "y_val6 = y_scaler6.transform(y_val6.values.reshape(-1, 1))\n",
    "y_test6 = y_scaler6.transform(y_test6.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd94a4b",
   "metadata": {},
   "source": [
    "### Create the training loop for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149354bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, criterion=None, optimizer=None, num_epochs=100, batch_size=32, v=True):\n",
    "    \"\"\"\n",
    "    Train the neural network model.\n",
    "    \"\"\"\n",
    "    \n",
    "    if criterion is None:\n",
    "        criterion = nn.MSELoss()\n",
    "    if optimizer is None:\n",
    "        optimizer = optim.SGD(model.parameters())\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    if v:\n",
    "        epoch_pbar = tqdm.tqdm(range(num_epochs), desc=\"Training\")\n",
    "    else:\n",
    "        epoch_pbar = range(num_epochs)\n",
    "\n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        permutation = torch.randperm(X_train.size()[0])\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        num_batches = (X_train.size()[0] + batch_size - 1) // batch_size\n",
    "        \n",
    "        for i in range(0, X_train.size()[0], batch_size):\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_X, batch_y = X_train[indices], y_train[indices]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / num_batches\n",
    "        history['train_loss'].append(avg_epoch_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val).item()\n",
    "            history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # Update progress bar with current metrics\n",
    "        if v:\n",
    "            epoch_pbar.set_postfix({\n",
    "                \"Train Loss\": f\"{avg_epoch_loss:.4f}\", \n",
    "                \"Val Loss\": f\"{val_loss:.4f}\"\n",
    "            })\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d13ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, problem=\"classification\", v=True):\n",
    "    \"\"\"\n",
    "    Evaluate the neural network model.\n",
    "    \"\"\"\n",
    "    if problem == \"classification\":\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.FloatTensor(X_test)\n",
    "            outputs = model(X_test_tensor)\n",
    "            \n",
    "            # one hot encoded categorical response vectors\n",
    "            if isinstance(y_test, np.ndarray) and len(y_test.shape) > 1:\n",
    "                # one hot encoded -> class labels\n",
    "                y_true = np.argmax(y_test, axis=1)\n",
    "                y_pred = np.argmax(outputs.cpu().numpy(), axis=1)\n",
    "            else:\n",
    "                y_true = y_test\n",
    "                y_pred = np.argmax(outputs.cpu().numpy(), axis=1)\n",
    "                \n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            \n",
    "            # Determine if binary or multiclass\n",
    "            n_classes = len(np.unique(y_true))\n",
    "            if n_classes == 2:\n",
    "                precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "                recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "                f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "            else:\n",
    "                precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "                recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "                f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            \n",
    "            mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "        if v:\n",
    "            print(\"\\nEvaluation on test set:\")\n",
    "            print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "            print(f'Precision: {precision:.4f}')\n",
    "            print(f'Recall: {recall:.4f}')\n",
    "            print(f'F1 Score: {f1:.4f}')\n",
    "            print(f'Matthews Correlation Coefficient: {mcc:.4f}')\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_true, y_pred))\n",
    "            print(\"Confusion Matrix:\")\n",
    "            print(confusion_matrix(y_true, y_pred))\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall, \n",
    "            'f1_score': f1,\n",
    "            'mcc': mcc\n",
    "        }\n",
    "    else:\n",
    "        # Regression\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.FloatTensor(X_test)\n",
    "            y_pred = model(X_test_tensor).cpu().numpy()\n",
    "            y_true = y_test\n",
    "            \n",
    "            # Ensure shapes match TODO\n",
    "            if len(y_pred.shape) > 1 and y_pred.shape[1] == 1:\n",
    "                y_pred = y_pred.flatten()\n",
    "            if len(y_true.shape) > 1 and y_true.shape[1] == 1:\n",
    "                y_true = y_true.flatten()\n",
    "            \n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            root_mse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            \n",
    "        if v:\n",
    "            print(\"\\nEvaluation on test set:\")\n",
    "            print(f'Test MSE: {mse:.4f}')\n",
    "            print(f\"Test Root MSE: {root_mse:.4f}\")\n",
    "            print(f'Test MAE: {mae:.4f}')\n",
    "            print(f'Test R-squared: {r2:.4f}')\n",
    "        \n",
    "        results = {\n",
    "            'mse': mse,\n",
    "            'root_mse': root_mse,\n",
    "            'mae': mae,\n",
    "            'r2_score': r2,\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc42bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, loss=\"MSE\"):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss over epochs.\n",
    "    \"\"\"\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(f'{loss} Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d08d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_pipeline(X, y, problem='classification', hidden_units=10, n_epochs=100, batch_size=32, learning_rate=0.001, momentum=0.9, weight_decay=1e-5, v=True):\n",
    "    \"\"\"\n",
    "    Training pipeline for classification and regression tasks. \n",
    "    \"\"\"\n",
    "    # unzip data\n",
    "    X_train, X_val, X_test = X\n",
    "    y_train, y_val, y_test = y\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    if problem == \"classification\":\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        n_output = y_train.shape[1]\n",
    "    else:\n",
    "        y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)\n",
    "        y_val_tensor = torch.FloatTensor(y_val).view(-1, 1)\n",
    "        n_output = 1\n",
    "  \n",
    "    n_input = X_train.shape[1]\n",
    "    n_hidden = hidden_units\n",
    "\n",
    "    model = NeuralNetwork(n_input, n_hidden, n_output)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    trained_model, history = train_model(\n",
    "        model, \n",
    "        X_train_tensor, \n",
    "        y_train_tensor, \n",
    "        X_val_tensor, \n",
    "        y_val_tensor, \n",
    "        criterion=criterion, \n",
    "        optimizer=optimizer, \n",
    "        num_epochs=n_epochs, \n",
    "        batch_size=batch_size,\n",
    "        v=v\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    results = evaluate_model(trained_model, X_test, y_test, problem=problem, v=v)\n",
    "    if v:\n",
    "        # Plot training and validation loss\n",
    "        plot_history(history, \"MSE\")\n",
    "    \n",
    "    return trained_model, history, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b8537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_cv(param_grid, X_train, y_train, problem='classification', cv=5):\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    param_combinations = list(product(*param_values))\n",
    "    \n",
    "    best_params = None\n",
    "    best_score = -np.inf\n",
    "    all_results = []\n",
    "    \n",
    "    if problem == 'classification':\n",
    "        y = np.argmax(y_train, axis=1)\n",
    "        kfold = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    else:\n",
    "        y = y_train\n",
    "        kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    for i, param_combo in enumerate(tqdm.tqdm(param_combinations, desc=\"Grid Search\")):\n",
    "        params = dict(zip(param_names, param_combo))\n",
    "        \n",
    "        cv_scores = []\n",
    "        cv_val_losses = []\n",
    "        cv_train_losses = []\n",
    "        try:\n",
    "            # Perform cross-validation\n",
    "            for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, y)):\n",
    "                # Split data for this fold\n",
    "                X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "                if isinstance(y_train, np.ndarray) and len(y_train.shape) > 1:\n",
    "                    y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "                else:\n",
    "                    if hasattr(y_train, 'iloc'):\n",
    "                        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "                    else:\n",
    "                        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "                \n",
    "                X_fold = (X_fold_train, X_fold_val, X_fold_val)\n",
    "                y_fold = (y_fold_train, y_fold_val, y_fold_val)\n",
    "                \n",
    "                _, history, results = training_pipeline(\n",
    "                        X_fold, y_fold,\n",
    "                        hidden_units=params['hidden_units'],\n",
    "                        problem=problem,\n",
    "                        n_epochs=params['n_epochs'],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        learning_rate=params['learning_rate'],\n",
    "                        momentum=params['momentum'],\n",
    "                        weight_decay=params['weight_decay'],\n",
    "                        v=False\n",
    "                    )\n",
    "                if problem == 'classification':\n",
    "                    fold_score = results['f1_score']\n",
    "                else:\n",
    "                    fold_score = results['r2_score']\n",
    "                \n",
    "                cv_scores.append(fold_score)\n",
    "                cv_val_losses.append(history['val_loss'][-1])\n",
    "                cv_train_losses.append(history['train_loss'][-1])\n",
    "            \n",
    "            # Calculate mean and std of CV scores\n",
    "            mean_score = np.mean(cv_scores)\n",
    "            std_score = np.std(cv_scores)\n",
    "            mean_val_loss = np.mean(cv_val_losses)\n",
    "            mean_train_loss = np.mean(cv_train_losses)\n",
    "            \n",
    "            # Store results\n",
    "            all_results.append({\n",
    "                'params': params.copy(),\n",
    "                'mean_score': mean_score,\n",
    "                'std_score': std_score,\n",
    "                'cv_scores': cv_scores,\n",
    "                'mean_val_loss': mean_val_loss,\n",
    "                'mean_train_loss': mean_train_loss\n",
    "            })\n",
    "            \n",
    "            # Update best parameters based on mean CV score\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params = params.copy()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with parameters {params}: {e}\")\n",
    "            continue\n",
    "          \n",
    "    return best_params, all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f1ee9b",
   "metadata": {},
   "source": [
    "## Passive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa32bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train1.shape)\n",
    "print(X_train2.shape)\n",
    "print(X_train3.shape)\n",
    "print(X_train4.shape)\n",
    "print(X_train5.shape)\n",
    "print(X_train6.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5cc3cd",
   "metadata": {},
   "source": [
    "#### Breast Cancer Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyper-parameter tuning\n",
    "# param_grid = {\n",
    "#     'hidden_units': [16],\n",
    "#     'n_epochs': [200, 300, 500],\n",
    "#     'batch_size': [16, 32],\n",
    "#     'learning_rate': [0.1, 0.01],\n",
    "#     'momentum': [0.8, 0.9],\n",
    "#     'weight_decay': [0.0001, 0.00001]\n",
    "# }\n",
    "\n",
    "# best_params1, all_results1 = grid_search_cv(param_grid, X_train1, y_train1, problem='classification')\n",
    "# print(best_params1)\n",
    "\n",
    "# Grid Search: 100%|██████████| 48/48 [23:55<00:00, 29.90s/it]\n",
    "# {'hidden_units': 16, 'n_epochs': 500, 'batch_size': 32, 'learning_rate': 0.1, 'momentum': 0.8, 'weight_decay': 0.0001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae75a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast cancer\n",
    "X = (X_train1, X_val1, X_test1)\n",
    "y = (y_train1, y_val1, y_test1)\n",
    "\n",
    "results_file = '../data/breast_cancer_passive_results.pkl'\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    final_performances = saved_data['final_performances']\n",
    "    model, history, results = training_pipeline(X, y, hidden_units=16, n_epochs=500, batch_size=32, learning_rate=0.1, momentum=0.8, weight_decay=0.0001, v=True)\n",
    "else:\n",
    "    final_performances = []\n",
    "    for i in tqdm.tqdm(range(9), desc=\"Running multiple independent runs:\"):\n",
    "        model, history, results = training_pipeline(X, y, hidden_units=16, n_epochs=500, batch_size=32, learning_rate=0.1, momentum=0.8, weight_decay=0.0001, v=False)\n",
    "        final_performances.append(results)\n",
    "        \n",
    "    model, history, results = training_pipeline(X, y, hidden_units=16, n_epochs=500, batch_size=32, learning_rate=0.1, momentum=0.8, weight_decay=0.0001, v=True)\n",
    "    final_performances.append(results)\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'hidden_units': 16,\n",
    "                'n_epochs': 500,\n",
    "                'batch_size': 32,\n",
    "                'learning_rate': 0.1,\n",
    "                'momentum': 0.8,\n",
    "                'weight_decay': 0.0001\n",
    "            }\n",
    "        }, f)\n",
    "        \n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE PASSIVE PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_accuracy = np.mean([p['accuracy'] for p in final_performances])\n",
    "std_accuracy = np.std([p['accuracy'] for p in final_performances])\n",
    "avg_f1 = np.mean([p['f1_score'] for p in final_performances])\n",
    "std_f1 = np.std([p['f1_score'] for p in final_performances])\n",
    "avg_precision = np.mean([p['precision'] for p in final_performances])\n",
    "std_precision = np.std([p['precision'] for p in final_performances])\n",
    "avg_recall = np.mean([p['recall'] for p in final_performances])\n",
    "std_recall = np.std([p['recall'] for p in final_performances])\n",
    "avg_mcc = np.mean([p['mcc'] for p in final_performances])\n",
    "std_mcc = np.std([p['mcc'] for p in final_performances])\n",
    "\n",
    "print(f\"Accuracy:  {avg_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "print(f\"F1 Score:  {avg_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Precision: {avg_precision:.4f} ± {std_precision:.4f}\")\n",
    "print(f\"Recall:    {avg_recall:.4f} ± {std_recall:.4f}\")\n",
    "print(f\"MCC:       {avg_mcc:.4f} ± {std_mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2277147",
   "metadata": {},
   "source": [
    "#### Mobile Pricing Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'hidden_units': [12],\n",
    "#     'n_epochs': [200, 500],\n",
    "#     'batch_size': [16, 32],\n",
    "#     'learning_rate': [0.5, 0.1, 0.01],\n",
    "#     'momentum': [0.8, 0.9],\n",
    "#     'weight_decay': [0.0001, 0.00001]\n",
    "# }\n",
    "\n",
    "# best_params2, all_results2 = grid_search_cv(param_grid, X_train2, y_train2, problem='classification')\n",
    "# print(best_params2)\n",
    "\n",
    "# Grid Search: 100%|██████████| 48/48 [45:09<00:00, 56.44s/it] \n",
    "# {'hidden_units': 12, 'n_epochs': 200, 'batch_size': 16, 'learning_rate': 0.1, 'momentum': 0.9, 'weight_decay': 1e-05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daa24e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mobile dataset\n",
    "X = (X_train2, X_val2, X_test2)\n",
    "y = (y_train2, y_val2, y_test2)\n",
    "\n",
    "results_file = '../data/mobile_pricing_passive_results.pkl'\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    final_performances = saved_data['final_performances']\n",
    "    model, history, results = training_pipeline(X, y, hidden_units=12, n_epochs=200, batch_size=16, learning_rate=0.1, momentum=0.9, weight_decay=1e-05)\n",
    "else:\n",
    "    final_performances = []\n",
    "    for i in tqdm.tqdm(range(9), desc=\"Running multiple independent runs:\"):\n",
    "        model, history, results = training_pipeline(X, y, hidden_units=12, n_epochs=200, batch_size=16, learning_rate=0.1, momentum=0.9, weight_decay=1e-05, v=False)\n",
    "        final_performances.append(results)\n",
    "        \n",
    "    model, history, results = training_pipeline(X, y, hidden_units=12, n_epochs=200, batch_size=16, learning_rate=0.1, momentum=0.9, weight_decay=1e-05)\n",
    "    final_performances.append(results)\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'hidden_units': 12,\n",
    "                'n_epochs': 200,\n",
    "                'batch_size': 16,\n",
    "                'learning_rate': 0.1,\n",
    "                'momentum': 0.9,\n",
    "                'weight_decay': 0.00001\n",
    "            }\n",
    "        }, f)\n",
    "        \n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE PASSIVE PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_accuracy = np.mean([p['accuracy'] for p in final_performances])\n",
    "std_accuracy = np.std([p['accuracy'] for p in final_performances])\n",
    "avg_f1 = np.mean([p['f1_score'] for p in final_performances])\n",
    "std_f1 = np.std([p['f1_score'] for p in final_performances])\n",
    "avg_precision = np.mean([p['precision'] for p in final_performances])\n",
    "std_precision = np.std([p['precision'] for p in final_performances])\n",
    "avg_recall = np.mean([p['recall'] for p in final_performances])\n",
    "std_recall = np.std([p['recall'] for p in final_performances])\n",
    "avg_mcc = np.mean([p['mcc'] for p in final_performances])\n",
    "std_mcc = np.std([p['mcc'] for p in final_performances])\n",
    "\n",
    "print(f\"Accuracy:  {avg_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "print(f\"F1 Score:  {avg_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Precision: {avg_precision:.4f} ± {std_precision:.4f}\")\n",
    "print(f\"Recall:    {avg_recall:.4f} ± {std_recall:.4f}\")\n",
    "print(f\"MCC:       {avg_mcc:.4f} ± {std_mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de5dff",
   "metadata": {},
   "source": [
    "#### Letter Recognitition Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'hidden_units': [24],\n",
    "#     'n_epochs': [1000, 1500],\n",
    "#     'batch_size': [16, 32],\n",
    "#     'learning_rate': [0.5, 0.1, 0.01],\n",
    "#     'momentum': [0.8, 0.9],\n",
    "#     'weight_decay': [0.0001, 0.00001]\n",
    "# }\n",
    "\n",
    "# best_params3, all_results3 = grid_search_cv(param_grid, X_train3, y_train3, problem='classification')\n",
    "# print(best_params3)\n",
    "\n",
    "# # Grid Search: 100%|██████████| 48/48 [1:15:18<00:00, 141.20s/it]\n",
    "# # {'hidden_units': 24, 'n_epochs': 1000, 'batch_size': 32, 'learning_rate': 0.5, 'momentum': 0.9, 'weight_decay': 1e-05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb715bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letter recognition\n",
    "X = (X_train3, X_val3, X_test3)\n",
    "y = (y_train3, y_val3, y_test3)\n",
    "\n",
    "results_file = '../data/letter_recognition_passive_results.pkl'\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    final_performances = saved_data['final_performances']\n",
    "    model, history, results = training_pipeline(X, y, hidden_units=24, n_epochs=1000, batch_size=32, learning_rate=0.5, momentum=0.9, weight_decay=1e-05)\n",
    "else:\n",
    "    final_performances = []\n",
    "    for i in tqdm.tqdm(range(9), desc=\"Running multiple independent runs:\"):\n",
    "        model, history, results = training_pipeline(X, y, hidden_units=24, n_epochs=1000, batch_size=32, learning_rate=0.5, momentum=0.9, weight_decay=1e-05, v=False)\n",
    "        final_performances.append(results)\n",
    "        \n",
    "    model, history, results = training_pipeline(X, y, hidden_units=24, n_epochs=1000, batch_size=32, learning_rate=0.5, momentum=0.9, weight_decay=1e-05)\n",
    "    final_performances.append(results)\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'hidden_units': 24,\n",
    "                'n_epochs': 1000,\n",
    "                'batch_size': 32,\n",
    "                'learning_rate': 0.5,\n",
    "                'momentum': 0.9,\n",
    "                'weight_decay': 0.00001\n",
    "            }\n",
    "        }, f)\n",
    "        \n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE PASSIVE PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_accuracy = np.mean([p['accuracy'] for p in final_performances])\n",
    "std_accuracy = np.std([p['accuracy'] for p in final_performances])\n",
    "avg_f1 = np.mean([p['f1_score'] for p in final_performances])\n",
    "std_f1 = np.std([p['f1_score'] for p in final_performances])\n",
    "avg_precision = np.mean([p['precision'] for p in final_performances])\n",
    "std_precision = np.std([p['precision'] for p in final_performances])\n",
    "avg_recall = np.mean([p['recall'] for p in final_performances])\n",
    "std_recall = np.std([p['recall'] for p in final_performances])\n",
    "avg_mcc = np.mean([p['mcc'] for p in final_performances])\n",
    "std_mcc = np.std([p['mcc'] for p in final_performances])\n",
    "\n",
    "print(f\"Accuracy:  {avg_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "print(f\"F1 Score:  {avg_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Precision: {avg_precision:.4f} ± {std_precision:.4f}\")\n",
    "print(f\"Recall:    {avg_recall:.4f} ± {std_recall:.4f}\")\n",
    "print(f\"MCC:       {avg_mcc:.4f} ± {std_mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d885bab",
   "metadata": {},
   "source": [
    "#### Housing Prices Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870f13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'hidden_units': [12],\n",
    "#     'n_epochs': [200, 300, 500],\n",
    "#     'batch_size': [16, 32],\n",
    "#     'learning_rate': [0.5, 0.1, 0.01],\n",
    "#     'momentum': [0.8, 0.9],\n",
    "#     'weight_decay': [0.0001, 0.00001]\n",
    "# }\n",
    "\n",
    "# best_params4, all_results4 = grid_search_cv(param_grid, X_train4, y_train4, problem='regression')\n",
    "# print(best_params4)\n",
    "\n",
    "# # Grid Search: 100%|██████████| 72/72 [11:34<00:00,  9.65s/it]\n",
    "# # {'hidden_units': 12, 'n_epochs': 500, 'batch_size': 16, 'learning_rate': 0.5, 'momentum': 0.8, 'weight_decay': 1e-05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b7115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housing prices\n",
    "X = (X_train4, X_val4, X_test4)\n",
    "y = (y_train4, y_val4, y_test4)\n",
    "\n",
    "results_file = '../data/boston_housing_passive_results.pkl'\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    final_performances = saved_data['final_performances']\n",
    "    model, history, results = training_pipeline(X, y, problem=\"regression\", hidden_units=12, n_epochs=500, batch_size=16, learning_rate=0.5, momentum=0.8, weight_decay=1e-05)\n",
    "else:\n",
    "    final_performances = []\n",
    "    for i in tqdm.tqdm(range(9), desc=\"Running multiple independent runs:\"):\n",
    "        model, history, results = training_pipeline(X, y, problem=\"regression\", hidden_units=12, n_epochs=500, batch_size=16, learning_rate=0.5, momentum=0.8, weight_decay=1e-05, v=False)\n",
    "        final_performances.append(results)\n",
    "        \n",
    "    model, history, results = training_pipeline(X, y, problem=\"regression\", hidden_units=12, n_epochs=500, batch_size=16, learning_rate=0.5, momentum=0.8, weight_decay=1e-05)\n",
    "    final_performances.append(results)\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'hidden_units': 12,\n",
    "                'n_epochs': 500,\n",
    "                'batch_size': 16,\n",
    "                'learning_rate': 0.5,\n",
    "                'momentum': 0.8,\n",
    "                'weight_decay': 0.00001\n",
    "            }\n",
    "        }, f)\n",
    "        \n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE PASSIVE PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_mse = np.mean([p['mse'] for p in final_performances])\n",
    "std_mse = np.std([p['mse'] for p in final_performances])\n",
    "avg_root_mse = np.mean([p['root_mse'] for p in final_performances])\n",
    "std_root_mse = np.std([p['root_mse'] for p in final_performances])\n",
    "avg_mae = np.mean([p['mae'] for p in final_performances])\n",
    "std_mae = np.std([p['mae'] for p in final_performances])\n",
    "avg_r2 = np.mean([p['r2_score'] for p in final_performances])\n",
    "std_r2 = np.std([p['r2_score'] for p in final_performances])\n",
    "\n",
    "print(f\"MSE: {avg_mse:.4f} ± {std_mse:.4f}\")\n",
    "print(f\"Root MSE: {avg_root_mse:.4f} ± {std_root_mse:.4f}\")\n",
    "print(f\"MAE: {avg_mae:.4f} ± {std_mae:.4f}\")\n",
    "print(f\"R²: {avg_r2:.4f} ± {std_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93051335",
   "metadata": {},
   "source": [
    "#### Concrete Compressive Strength Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477faf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'hidden_units': [8],\n",
    "#     'n_epochs': [200, 500],\n",
    "#     'batch_size': [16, 32],\n",
    "#     'learning_rate': [0.5, 0.1, 0.01],\n",
    "#     'momentum': [0.8, 0.9],\n",
    "#     'weight_decay': [0.0001, 0.00001]\n",
    "# }\n",
    "\n",
    "# best_params5, all_results5 = grid_search_cv(param_grid, X_train5, y_train5, problem='regression')\n",
    "# print(best_params5)\n",
    "\n",
    "# # Grid Search: 100%|██████████| 48/48 [17:14<00:00, 21.55s/it]\n",
    "# # {'hidden_units': 8, 'n_epochs': 500, 'batch_size': 16, 'learning_rate': 0.5, 'momentum': 0.8, 'weight_decay': 1e-05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ba344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concrete dataset\n",
    "X = (X_train5, X_val5, X_test5)\n",
    "y = (y_train5, y_val5, y_test5)\n",
    "\n",
    "results_file = '../data/concrete_strength_passive_results.pkl'\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    final_performances = saved_data['final_performances']\n",
    "    model, history, results = training_pipeline(X, y, problem=\"regression\", hidden_units=8, n_epochs=500, batch_size=16, learning_rate=0.5, momentum=0.8, weight_decay=1e-05)\n",
    "else:\n",
    "    final_performances = []\n",
    "    for i in tqdm.tqdm(range(9), desc=\"Running multiple independent runs:\"):\n",
    "        model, history, results = training_pipeline(X, y, problem=\"regression\", hidden_units=8, n_epochs=500, batch_size=16, learning_rate=0.5, momentum=0.8, weight_decay=1e-05, v=False)\n",
    "        final_performances.append(results)\n",
    "        \n",
    "    model, history, results = training_pipeline(X, y, problem=\"regression\", hidden_units=8, n_epochs=500, batch_size=16, learning_rate=0.5, momentum=0.8, weight_decay=1e-05)\n",
    "    final_performances.append(results)\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'hidden_units': 8,\n",
    "                'n_epochs': 500,\n",
    "                'batch_size': 16,\n",
    "                'learning_rate': 0.5,\n",
    "                'momentum': 0.8,\n",
    "                'weight_decay': 0.00001\n",
    "            }\n",
    "        }, f)\n",
    "        \n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE PASSIVE PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_mse = np.mean([p['mse'] for p in final_performances])\n",
    "std_mse = np.std([p['mse'] for p in final_performances])\n",
    "avg_root_mse = np.mean([p['root_mse'] for p in final_performances])\n",
    "std_root_mse = np.std([p['root_mse'] for p in final_performances])\n",
    "avg_mae = np.mean([p['mae'] for p in final_performances])\n",
    "std_mae = np.std([p['mae'] for p in final_performances])\n",
    "avg_r2 = np.mean([p['r2_score'] for p in final_performances])\n",
    "std_r2 = np.std([p['r2_score'] for p in final_performances])\n",
    "\n",
    "print(f\"MSE: {avg_mse:.4f} ± {std_mse:.4f}\")\n",
    "print(f\"Root MSE: {avg_root_mse:.4f} ± {std_root_mse:.4f}\")\n",
    "print(f\"MAE: {avg_mae:.4f} ± {std_mae:.4f}\")\n",
    "print(f\"R²: {avg_r2:.4f} ± {std_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7eae5d",
   "metadata": {},
   "source": [
    "#### Abalone Age Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed46507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'hidden_units': [16],\n",
    "#     'n_epochs': [200, 500],\n",
    "#     'batch_size': [16, 32],\n",
    "#     'learning_rate': [0.5, 0.1, 0.01],\n",
    "#     'momentum': [0.8, 0.9],\n",
    "#     'weight_decay': [0.0001, 0.00001]\n",
    "# }\n",
    "\n",
    "# best_params6, all_results6 = grid_search_cv(param_grid, X_train6, y_train6, problem='regression')\n",
    "# print(best_params6)\n",
    "\n",
    "# # Grid Search: 100%|██████████| 48/48 [1:02:48<00:00, 78.52s/it]\n",
    "# # {'hidden_units': 16, 'n_epochs': 500, 'batch_size': 16, 'learning_rate': 0.01, 'momentum': 0.9, 'weight_decay': 1e-05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db46a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abalone dataset\n",
    "X = (X_train6, X_val6, X_test6)\n",
    "y = (y_train6, y_val6, y_test6)\n",
    "\n",
    "results_file = '../data/abalone_age_passive_results.pkl'\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    final_performances = saved_data['final_performances']\n",
    "    model, history, results = training_pipeline(X, y, problem=\"regression\", hidden_units=16, n_epochs=500, batch_size=16, learning_rate=0.05, momentum=0.9, weight_decay=1e-04)\n",
    "else:\n",
    "    final_performances = []\n",
    "    for i in tqdm.tqdm(range(9), desc=\"Running multiple independent runs:\"):\n",
    "        model, history, results = training_pipeline(X, y, problem=\"regression\", hidden_units=16, n_epochs=500, batch_size=16, learning_rate=0.05, momentum=0.9, weight_decay=1e-04, v=False)\n",
    "        final_performances.append(results)\n",
    "\n",
    "    model, history, results = training_pipeline(X, y, problem=\"regression\", hidden_units=16, n_epochs=500, batch_size=16, learning_rate=0.05, momentum=0.9, weight_decay=1e-04)\n",
    "    final_performances.append(results)\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'hidden_units': 16,\n",
    "                'n_epochs': 500,\n",
    "                'batch_size': 16,\n",
    "                'learning_rate': 0.05,\n",
    "                'momentum': 0.9,\n",
    "                'weight_decay': 0.0001\n",
    "            }\n",
    "        }, f)\n",
    "        \n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE PASSIVE PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_mse = np.mean([p['mse'] for p in final_performances])\n",
    "std_mse = np.std([p['mse'] for p in final_performances])\n",
    "avg_root_mse = np.mean([p['root_mse'] for p in final_performances])\n",
    "std_root_mse = np.std([p['root_mse'] for p in final_performances])\n",
    "avg_mae = np.mean([p['mae'] for p in final_performances])\n",
    "std_mae = np.std([p['mae'] for p in final_performances])\n",
    "avg_r2 = np.mean([p['r2_score'] for p in final_performances])\n",
    "std_r2 = np.std([p['r2_score'] for p in final_performances])\n",
    "\n",
    "print(f\"MSE: {avg_mse:.4f} ± {std_mse:.4f}\")\n",
    "print(f\"Root MSE: {avg_root_mse:.4f} ± {std_root_mse:.4f}\")\n",
    "print(f\"MAE: {avg_mae:.4f} ± {std_mae:.4f}\")\n",
    "print(f\"R²: {avg_r2:.4f} ± {std_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cc755c",
   "metadata": {},
   "source": [
    "## Active Learning: Uncertainty Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d789512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_sampling_classification(model, X_unlabeled, n_samples=1):\n",
    "    \"\"\"\n",
    "    Select most uncertain sample for classification for entropy uncertainty method.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X_unlabeled)\n",
    "        outputs = model(X_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "    # Shannon entropy - higher entropy = more uncertainty\n",
    "    uncertainty = -np.sum(probs * np.log(probs + 1e-10), axis=1)\n",
    "    # Select most uncertain samples\n",
    "    uncertain_indices = np.argsort(uncertainty)[-n_samples:]\n",
    "\n",
    "    return uncertain_indices, uncertainty[uncertain_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_sampling_regression(models, X_unlabeled, n_samples=1):\n",
    "    \"\"\"\n",
    "    Select most uncertain samples for regression using prediction variance.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    # Get predictions from all models in ensemble\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X_unlabeled)\n",
    "            outputs = model(X_tensor).cpu().numpy().flatten()\n",
    "            predictions.append(outputs)\n",
    "    \n",
    "    # Stack predictions: [n_models, n_samples]\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Calculate variance across ensemble predictions for each sample\n",
    "    uncertainty_scores = np.var(predictions, axis=0)\n",
    "    \n",
    "    # Select samples with highest variance (most uncertain)\n",
    "    uncertain_indices = np.argsort(uncertainty_scores)[-n_samples:]\n",
    "    \n",
    "    return uncertain_indices, uncertainty_scores[uncertain_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e3c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble_models(X_data, y_data, n_models=5, **model_params):\n",
    "    \"\"\"\n",
    "    Train an ensemble of models with different random seeds for uncertainty estimation.\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        # Set different random seed for each model\n",
    "        torch.manual_seed(42 + i)\n",
    "        np.random.seed(42 + i)\n",
    "        \n",
    "        model, history, results = training_pipeline(\n",
    "            X_data, y_data, problem=\"regression\", v=False, **model_params\n",
    "        )\n",
    "        \n",
    "        models.append(model)\n",
    "    \n",
    "    return models, history, results # return last model's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2bde5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_sampling_pipeline(X, y, X_pool, y_pool, problem=\"classification\", budget=100, n_samples=1, **model_params):\n",
    "    \"\"\"\n",
    "    Active learning pipeline.\n",
    "    \"\"\"\n",
    "    results_history = []\n",
    "    \n",
    "    # unpack data\n",
    "    X_train, X_val, X_test = X\n",
    "    y_train, y_val, y_test = y\n",
    "    \n",
    "    # Initial labeled training dataset\n",
    "    X_train_current = X_train.copy()\n",
    "    y_train_current = y_train.copy()\n",
    "    \n",
    "    # The unlabeled data pool\n",
    "    X_pool = X_pool.copy()\n",
    "    y_pool = y_pool.copy()     # Simulated acquired labels with real labels\n",
    "    \n",
    "    pbar = tqdm.tqdm(range(budget), desc=\"Uncertainty Sampling\")\n",
    "    for iteration in pbar:\n",
    "        # Train model on current labeled data\n",
    "        X_data = (X_train_current, X_val, X_test)\n",
    "        y_data = (y_train_current, y_val, y_test)\n",
    "        \n",
    "        if problem == \"classification\":\n",
    "            model, history, results = training_pipeline(\n",
    "                X_data, y_data, problem=problem, v=False, **model_params\n",
    "            )\n",
    "            # Store results\n",
    "            results_history.append({\n",
    "                'iteration': iteration + 1,\n",
    "                'training_size': len(X_train_current),\n",
    "                'f1_score': results['f1_score'],\n",
    "                'accuracy': results['accuracy'],\n",
    "                'precision': results['precision'],\n",
    "                'recall': results['recall'],\n",
    "                'mcc': results['mcc']\n",
    "            })\n",
    "            pbar.set_postfix({\n",
    "                \"Train Size\": len(X_train_current),\n",
    "                \"Pool Size\": len(X_pool),\n",
    "                \"F1\": f\"{results['f1_score']:.3f}\",\n",
    "                \"Acc\": f\"{results['accuracy']:.3f}\"\n",
    "            })\n",
    "        else:\n",
    "            models, history, results = train_ensemble_models(\n",
    "                X_data, y_data,\n",
    "                n_models=5, **model_params\n",
    "            )\n",
    "            model = models[0]  # Use the first model for evaluation\n",
    "            results_history.append({\n",
    "                'iteration': iteration + 1,\n",
    "                'training_size': len(X_train_current),\n",
    "                'r2_score': results['r2_score'],\n",
    "                'mse': results['mse'],\n",
    "                'root_mse': results['root_mse'],\n",
    "                'mae': results['mae']\n",
    "            })\n",
    "            pbar.set_postfix({\n",
    "                \"Train Size\": len(X_train_current),\n",
    "                \"Pool Size\": len(X_pool),\n",
    "                \"R2\": f\"{results['r2_score']:.3f}\",\n",
    "                \"MSE\": f\"{results['mse']:.4f}\"\n",
    "            })\n",
    "        \n",
    "        # select new sample\n",
    "        if iteration < budget - 1 and len(X_pool) > 0:\n",
    "            # Select uncrtain samples based on problem type\n",
    "            if problem == \"classification\":\n",
    "                uncertain_indices, uncertainty_score = uncertainty_sampling_classification(model, X_pool, n_samples=n_samples)\n",
    "            else:\n",
    "                uncertain_indices, uncertainty_score = uncertainty_sampling_regression(models, X_pool, n_samples=n_samples)\n",
    "\n",
    "            # move selected samples from pool to training set\n",
    "            if isinstance(X_pool, np.ndarray):\n",
    "                new_X = X_pool[uncertain_indices]\n",
    "                X_pool = np.delete(X_pool, uncertain_indices, axis=0)\n",
    "            else:\n",
    "                new_X = X_pool.iloc[uncertain_indices].values\n",
    "                X_pool = X_pool.drop(X_pool.index[uncertain_indices]).reset_index(drop=True)\n",
    "\n",
    "            # Obtain the label\n",
    "            if isinstance(y_pool, np.ndarray):\n",
    "                new_y = y_pool[uncertain_indices]\n",
    "                y_pool = np.delete(y_pool, uncertain_indices, axis=0)\n",
    "            else:\n",
    "                new_y = y_pool.iloc[uncertain_indices].values\n",
    "                y_pool = y_pool.drop(y_pool.index[uncertain_indices]).reset_index(drop=True)\n",
    "\n",
    "            # Add to training set - handle both numpy arrays and pandas objects\n",
    "            if isinstance(X_train_current, np.ndarray):\n",
    "                X_train_current = np.vstack([X_train_current, new_X])\n",
    "            else:\n",
    "                X_train_current = pd.concat([X_train_current, pd.DataFrame(new_X)], ignore_index=True)\n",
    "                \n",
    "            if isinstance(y_train_current, np.ndarray):\n",
    "                y_train_current = np.vstack([y_train_current, new_y])\n",
    "            else:\n",
    "                y_train_current = pd.concat([y_train_current, pd.Series(new_y)], ignore_index=True)\n",
    "    \n",
    "    plot_history(history) # plot history of final model\n",
    "    \n",
    "    return results_history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f58c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_sampling(X, y, problem='classification', budget=100, n_samples=1, **model_params):\n",
    "    \"\"\"\n",
    "    Uncertainty sampling active learning for classification and regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Data processing\n",
    "    X_train, X_val, X_test = X\n",
    "    y_train, y_val, y_test = y\n",
    "        \n",
    "    # randomly sample n_samples from the pool to form the initial training set\n",
    "    initial_indices = np.random.choice(len(X_train), size=n_samples, replace=False)\n",
    "    X_train_initial = X_train[initial_indices]\n",
    "    y_train_initial = y_train[initial_indices]\n",
    "    # Remove initial samples from the pool\n",
    "    X_pool = np.delete(X_train, initial_indices, axis=0)\n",
    "    y_pool = np.delete(y_train, initial_indices, axis=0)\n",
    "        \n",
    "    X_data = (X_train_initial, X_val, X_test)\n",
    "    y_data = (y_train_initial, y_val, y_test)\n",
    "    \n",
    "    results_history, model = uncertainty_sampling_pipeline(\n",
    "        X_data, y_data, X_pool, y_pool, problem=problem, budget=budget, n_samples=n_samples, **model_params\n",
    "    )\n",
    "  \n",
    "    return results_history, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b5b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_active_learning_results(all_results, problem='classification', saveas=None):\n",
    "    # Prepare data for visualization\n",
    "    training_sizes = [r['training_size'] for r in all_results[0]]\n",
    "\n",
    "    if problem == 'classification':\n",
    "        f1_by_size = {size: [] for size in training_sizes}\n",
    "\n",
    "        for run_results in all_results:\n",
    "            for result in run_results:\n",
    "                size = result['training_size']\n",
    "                f1_by_size[size].append(result['f1_score'])\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        sizes = sorted(f1_by_size.keys())\n",
    "        f1_means = [np.mean(f1_by_size[size]) for size in sizes]\n",
    "        f1_se = [np.std(f1_by_size[size]) / np.sqrt(len(f1_by_size[size])) for size in sizes]\n",
    "\n",
    "        # Create the plot with error bars\n",
    "        fig, ax = plt.subplots(figsize=(column_width_inches, column_width_inches * 0.75))\n",
    "\n",
    "        # F1 Score plot with error bars\n",
    "        ax.errorbar(sizes, f1_means, yerr=f1_se, fmt='o-', color='darkblue', \n",
    "                    linewidth=2, markersize=4, capsize=3, capthick=1)\n",
    "        ax.set_xlabel('Training Set Size', fontsize=16)\n",
    "        ax.set_ylabel('F1 Score', fontsize=16)\n",
    "        ax.grid(True)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "    else:  # regression\n",
    "        r2_by_size = {size: [] for size in training_sizes}\n",
    "\n",
    "        for run_results in all_results:\n",
    "            for result in run_results:\n",
    "                size = result['training_size']\n",
    "                r2_by_size[size].append(result['r2_score'])\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        sizes = sorted(r2_by_size.keys())\n",
    "        r2_means = [np.mean(r2_by_size[size]) for size in sizes]\n",
    "        r2_se = [np.std(r2_by_size[size]) / np.sqrt(len(r2_by_size[size])) for size in sizes]\n",
    "\n",
    "        # Create the plot with error bars\n",
    "        fig, ax = plt.subplots(figsize=(column_width_inches, column_width_inches * 0.75))\n",
    "\n",
    "        # R2 Score plot with error bars\n",
    "        ax.errorbar(sizes, r2_means, yerr=r2_se, fmt='o-', color='red', \n",
    "                    linewidth=2, markersize=4, capsize=3, capthick=1)\n",
    "\n",
    "        ax.set_xlabel('Training Set Size', fontsize=16)\n",
    "        ax.set_ylabel('R-squared Score', fontsize=16)\n",
    "        ax.grid(True)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if saveas is not None:\n",
    "        plt.savefig(f'../report/plots/{saveas}.pdf', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06b805",
   "metadata": {},
   "source": [
    "#### Breast Cancer Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e317b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The original training set size for breast cancer classification is {X_train1.shape[0]} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = (int)(0.8 * len(X_train1))  # consider at most 80% of original training data\n",
    "n_samples = max_samples // 25  # number of samples to acquire in each iteration\n",
    "budget = max_samples // n_samples  # number of iterations\n",
    "\n",
    "# Define the results file path\n",
    "results_file = '../data/breast_cancer_uncertainty_results.pkl'\n",
    "\n",
    "# Check if results already exist\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    \n",
    "    all_results = saved_data['all_results']\n",
    "    final_performances = saved_data['final_performances']\n",
    "else:\n",
    "    all_results = []\n",
    "    final_performances = []\n",
    "    \n",
    "    # Perform ten independent runs\n",
    "    for i in range(10):\n",
    "        np.random.seed(42 + i)\n",
    "        torch.manual_seed(42 + i)\n",
    "        # Breast Cancer Active Learning\n",
    "        active_results, final_model = uncertainty_sampling(\n",
    "            (X_train1, X_val1, X_test1),\n",
    "            (y_train1, y_val1, y_test1),\n",
    "            problem=\"classification\",\n",
    "            budget=budget,\n",
    "            n_samples=n_samples,\n",
    "            hidden_units=16,\n",
    "            n_epochs=500,\n",
    "            batch_size=32,\n",
    "            learning_rate=0.1,\n",
    "            momentum=0.8,\n",
    "            weight_decay=0.0001,\n",
    "        )\n",
    "        all_results.append(active_results)\n",
    "        \n",
    "        # Store final performance for this run\n",
    "        final_result = active_results[-1]\n",
    "        final_performances.append({\n",
    "            'accuracy': final_result['accuracy'],\n",
    "            'f1_score': final_result['f1_score'],\n",
    "            'precision': final_result['precision'],\n",
    "            'recall': final_result['recall'],\n",
    "            'training_size': final_result['training_size']\n",
    "        })\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'all_results': all_results,\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'max_samples': max_samples,\n",
    "                'n_samples': n_samples,\n",
    "                'budget': budget,\n",
    "                'hidden_units': 16,\n",
    "                'n_epochs': 500,\n",
    "                'batch_size': 32,\n",
    "                'learning_rate': 0.1,\n",
    "                'momentum': 0.8,\n",
    "                'weight_decay': 0.0001\n",
    "            }\n",
    "        }, f)\n",
    "\n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE FINAL PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_accuracy = np.mean([p['accuracy'] for p in final_performances])\n",
    "std_accuracy = np.std([p['accuracy'] for p in final_performances])\n",
    "avg_f1 = np.mean([p['f1_score'] for p in final_performances])\n",
    "std_f1 = np.std([p['f1_score'] for p in final_performances])\n",
    "avg_precision = np.mean([p['precision'] for p in final_performances])\n",
    "std_precision = np.std([p['precision'] for p in final_performances])\n",
    "avg_recall = np.mean([p['recall'] for p in final_performances])\n",
    "std_recall = np.std([p['recall'] for p in final_performances])\n",
    "print(f\"Accuracy:  {avg_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "print(f\"F1 Score:  {avg_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Precision: {avg_precision:.4f} ± {std_precision:.4f}\")\n",
    "print(f\"Recall:    {avg_recall:.4f} ± {std_recall:.4f}\")\n",
    "\n",
    "plot_active_learning_results(all_results, problem='classification', saveas='breast_cancer_uncertainty_sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f8bde",
   "metadata": {},
   "source": [
    "#### Mobile Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = (int)(0.8 * len(X_train2))  # consider at most 80% of original training data\n",
    "n_samples = max_samples // 25  # number of samples to acquire in each iteration\n",
    "budget = max_samples // n_samples  # number of iterations\n",
    "\n",
    "# Define the results file path\n",
    "results_file = '../data/mobile_pricing_uncertainty_results.pkl'\n",
    "\n",
    "# Check if results already exist\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    \n",
    "    all_results = saved_data['all_results']\n",
    "    final_performances = saved_data['final_performances']\n",
    "else:\n",
    "    all_results = []\n",
    "    final_performances = []\n",
    "    \n",
    "    # Perform ten independent runs\n",
    "    for i in range(10):\n",
    "        np.random.seed(42 + i)\n",
    "        torch.manual_seed(42 + i)\n",
    "        # Mobile Classification Active Learning\n",
    "        active_results, final_model = uncertainty_sampling(\n",
    "            (X_train2, X_val2, X_test2),\n",
    "            (y_train2, y_val2, y_test2),\n",
    "            problem=\"classification\",\n",
    "            budget=budget,\n",
    "            n_samples=n_samples,\n",
    "            hidden_units=12,\n",
    "            n_epochs=200,\n",
    "            batch_size=16,\n",
    "            learning_rate=0.1,\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-05,\n",
    "        )\n",
    "        all_results.append(active_results)\n",
    "        \n",
    "        # Store final performance for this run\n",
    "        final_result = active_results[-1]\n",
    "        final_performances.append({\n",
    "            'accuracy': final_result['accuracy'],\n",
    "            'f1_score': final_result['f1_score'],\n",
    "            'precision': final_result['precision'],\n",
    "            'recall': final_result['recall'],\n",
    "            'training_size': final_result['training_size']\n",
    "        })\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'all_results': all_results,\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'max_samples': max_samples,\n",
    "                'n_samples': n_samples,\n",
    "                'budget': budget,\n",
    "                'hidden_units': 12,\n",
    "                'n_epochs': 200,\n",
    "                'batch_size': 16,\n",
    "                'learning_rate': 0.1,\n",
    "                'momentum': 0.9,\n",
    "                'weight_decay': 1e-05\n",
    "            }\n",
    "        }, f)\n",
    "\n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE FINAL PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_accuracy = np.mean([p['accuracy'] for p in final_performances])\n",
    "std_accuracy = np.std([p['accuracy'] for p in final_performances])\n",
    "avg_f1 = np.mean([p['f1_score'] for p in final_performances])\n",
    "std_f1 = np.std([p['f1_score'] for p in final_performances])\n",
    "avg_precision = np.mean([p['precision'] for p in final_performances])\n",
    "std_precision = np.std([p['precision'] for p in final_performances])\n",
    "avg_recall = np.mean([p['recall'] for p in final_performances])\n",
    "std_recall = np.std([p['recall'] for p in final_performances])\n",
    "print(f\"Accuracy:  {avg_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "print(f\"F1 Score:  {avg_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Precision: {avg_precision:.4f} ± {std_precision:.4f}\")\n",
    "print(f\"Recall:    {avg_recall:.4f} ± {std_recall:.4f}\")\n",
    "\n",
    "plot_active_learning_results(all_results, problem='classification', saveas='mobile_pricing_uncertainty_sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd2cb6",
   "metadata": {},
   "source": [
    "#### Letter Recognition Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485569c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = (int)(0.8 * len(X_train3))\n",
    "n_samples = max_samples // 25\n",
    "budget = max_samples // n_samples \n",
    "\n",
    "# Define the results file path\n",
    "results_file = '../data/letter_recognition_uncertainty_results.pkl'\n",
    "\n",
    "# Check if results already exist\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    \n",
    "    all_results = saved_data['all_results']\n",
    "    final_performances = saved_data['final_performances']\n",
    "else:\n",
    "    all_results = []\n",
    "    final_performances = []\n",
    "    \n",
    "    # Perform ten independent runs\n",
    "    for i in range(10):\n",
    "        np.random.seed(42 + i)\n",
    "        torch.manual_seed(42 + i)\n",
    "        # Letter Recognition Active Learning\n",
    "        active_results, final_model = uncertainty_sampling(\n",
    "            (X_train3, X_val3, X_test3),\n",
    "            (y_train3, y_val3, y_test3),\n",
    "            problem=\"classification\",\n",
    "            budget=budget,\n",
    "            n_samples=n_samples,\n",
    "            hidden_units=24,\n",
    "            n_epochs=1000,\n",
    "            batch_size=32,\n",
    "            learning_rate=0.5,\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-05,\n",
    "        )\n",
    "        all_results.append(active_results)\n",
    "        \n",
    "        # Store final performance for this run\n",
    "        final_result = active_results[-1]\n",
    "        final_performances.append({\n",
    "            'accuracy': final_result['accuracy'],\n",
    "            'f1_score': final_result['f1_score'],\n",
    "            'precision': final_result['precision'],\n",
    "            'recall': final_result['recall'],\n",
    "            'training_size': final_result['training_size']\n",
    "        })\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'all_results': all_results,\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'max_samples': max_samples,\n",
    "                'n_samples': n_samples,\n",
    "                'budget': budget,\n",
    "                'hidden_units': 24,\n",
    "                'n_epochs': 1000,\n",
    "                'batch_size': 32,\n",
    "                'learning_rate': 0.5,\n",
    "                'momentum': 0.9,\n",
    "                'weight_decay': 1e-05\n",
    "            }\n",
    "        }, f)\n",
    "\n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE FINAL PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_accuracy = np.mean([p['accuracy'] for p in final_performances])\n",
    "std_accuracy = np.std([p['accuracy'] for p in final_performances])\n",
    "avg_f1 = np.mean([p['f1_score'] for p in final_performances])\n",
    "std_f1 = np.std([p['f1_score'] for p in final_performances])\n",
    "avg_precision = np.mean([p['precision'] for p in final_performances])\n",
    "std_precision = np.std([p['precision'] for p in final_performances])\n",
    "avg_recall = np.mean([p['recall'] for p in final_performances])\n",
    "std_recall = np.std([p['recall'] for p in final_performances])\n",
    "print(f\"Accuracy:  {avg_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "print(f\"F1 Score:  {avg_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Precision: {avg_precision:.4f} ± {std_precision:.4f}\")\n",
    "print(f\"Recall:    {avg_recall:.4f} ± {std_recall:.4f}\")\n",
    "\n",
    "plot_active_learning_results(all_results, problem='classification', saveas='letter_recognition_uncertainty_sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6692e2b6",
   "metadata": {},
   "source": [
    "#### Boston Housing Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5af147",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edf67b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = (int)(0.8 * len(X_train4))\n",
    "n_samples = max_samples // 25\n",
    "budget = max_samples // n_samples\n",
    "\n",
    "# Define the results file path\n",
    "results_file = '../data/boston_housing_uncertainty_results.pkl'\n",
    "\n",
    "# Check if results already exist\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    \n",
    "    all_results = saved_data['all_results']\n",
    "    final_performances = saved_data['final_performances']\n",
    "else:\n",
    "    all_results = []\n",
    "    final_performances = []\n",
    "    \n",
    "    # Perform ten independent runs\n",
    "    for i in range(10):\n",
    "        np.random.seed(42 + i)\n",
    "        torch.manual_seed(42 + i)\n",
    "        # Boston housing\n",
    "        active_results, final_model = uncertainty_sampling(\n",
    "            (X_train4, X_val4, X_test4),\n",
    "            (y_train4, y_val4, y_test4),\n",
    "            problem=\"regression\",\n",
    "            budget=budget,\n",
    "            n_samples=n_samples,\n",
    "            hidden_units=12,\n",
    "            n_epochs=500,\n",
    "            batch_size=16,\n",
    "            learning_rate=0.5,\n",
    "            momentum=0.8,\n",
    "            weight_decay=1e-05,\n",
    "        )\n",
    "        all_results.append(active_results)\n",
    "        \n",
    "        # Store final performance for this run\n",
    "        final_result = active_results[-1]\n",
    "        final_performances.append({\n",
    "            'mse': final_result['mse'],\n",
    "            'root_mse': final_result['root_mse'],\n",
    "            'mae': final_result['mae'],\n",
    "            'r2_score': final_result['r2_score'],\n",
    "            'training_size': final_result['training_size']\n",
    "        })\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'all_results': all_results,\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'max_samples': max_samples,\n",
    "                'n_samples': n_samples,\n",
    "                'budget': budget,\n",
    "                'hidden_units': 12,\n",
    "                'n_epochs': 500,\n",
    "                'batch_size': 16,\n",
    "                'learning_rate': 0.5,\n",
    "                'momentum': 0.8,\n",
    "                'weight_decay': 1e-05\n",
    "            }\n",
    "        }, f)\n",
    "\n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE FINAL PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_mse = np.mean([p['mse'] for p in final_performances])\n",
    "std_mse = np.std([p['mse'] for p in final_performances])\n",
    "avg_root_mse = np.mean([p['root_mse'] for p in final_performances])\n",
    "std_root_mse = np.std([p['root_mse'] for p in final_performances])\n",
    "avg_mae = np.mean([p['mae'] for p in final_performances])\n",
    "std_mae = np.std([p['mae'] for p in final_performances])\n",
    "avg_r2 = np.mean([p['r2_score'] for p in final_performances])\n",
    "std_r2 = np.std([p['r2_score'] for p in final_performances])\n",
    "print(f\"MSE: {avg_mse:.4f} ± {std_mse:.4f}\")\n",
    "print(f\"Root MSE: {avg_root_mse:.4f} ± {std_root_mse:.4f}\")\n",
    "print(f\"MAE: {avg_mae:.4f} ± {std_mae:.4f}\")\n",
    "print(f\"R²: {avg_r2:.4f} ± {std_r2:.4f}\")\n",
    "\n",
    "plot_active_learning_results(all_results, problem='regression', saveas='boston_housing_uncertainty_sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a32915b",
   "metadata": {},
   "source": [
    "#### Concrete Compressive Strength Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b379270",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c643b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = (int)(0.8 * len(X_train5))\n",
    "n_samples = max_samples // 25\n",
    "budget = max_samples // n_samples\n",
    "\n",
    "# Define the results file path\n",
    "results_file = '../data/concrete_strength_uncertainty_results.pkl'\n",
    "\n",
    "# Check if results already exist\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    \n",
    "    all_results = saved_data['all_results']\n",
    "    final_performances = saved_data['final_performances']\n",
    "else:\n",
    "    all_results = []\n",
    "    final_performances = []\n",
    "    \n",
    "    # Perform ten independent runs\n",
    "    for i in range(10):\n",
    "        np.random.seed(42 + i)\n",
    "        torch.manual_seed(42 + i)\n",
    "        # Concrete Strength Active Learning\n",
    "        active_results, final_model = uncertainty_sampling(\n",
    "            (X_train5, X_val5, X_test5),\n",
    "            (y_train5, y_val5, y_test5),\n",
    "            problem=\"regression\",\n",
    "            budget=budget,\n",
    "            n_samples=n_samples,\n",
    "            hidden_units=8,\n",
    "            n_epochs=500,\n",
    "            batch_size=16,\n",
    "            learning_rate=0.5,\n",
    "            momentum=0.8,\n",
    "            weight_decay=1e-05,\n",
    "        )\n",
    "        all_results.append(active_results)\n",
    "        \n",
    "        # Store final performance for this run\n",
    "        final_result = active_results[-1]\n",
    "        final_performances.append({\n",
    "            'mse': final_result['mse'],\n",
    "            'root_mse': final_result['root_mse'],\n",
    "            'mae': final_result['mae'],\n",
    "            'r2_score': final_result['r2_score'],\n",
    "            'training_size': final_result['training_size']\n",
    "        })\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'all_results': all_results,\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'max_samples': max_samples,\n",
    "                'n_samples': n_samples,\n",
    "                'budget': budget,\n",
    "                'hidden_units': 8,\n",
    "                'n_epochs': 500,\n",
    "                'batch_size': 16,\n",
    "                'learning_rate': 0.5,\n",
    "                'momentum': 0.8,\n",
    "                'weight_decay': 1e-05\n",
    "            }\n",
    "        }, f)\n",
    "\n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE FINAL PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_mse = np.mean([p['mse'] for p in final_performances])\n",
    "std_mse = np.std([p['mse'] for p in final_performances])\n",
    "avg_root_mse = np.mean([p['root_mse'] for p in final_performances])\n",
    "std_root_mse = np.std([p['root_mse'] for p in final_performances])\n",
    "avg_mae = np.mean([p['mae'] for p in final_performances])\n",
    "std_mae = np.std([p['mae'] for p in final_performances])\n",
    "avg_r2 = np.mean([p['r2_score'] for p in final_performances])\n",
    "std_r2 = np.std([p['r2_score'] for p in final_performances])\n",
    "print(f\"MSE: {avg_mse:.4f} ± {std_mse:.4f}\")\n",
    "print(f\"Root MSE: {avg_root_mse:.4f} ± {std_root_mse:.4f}\")\n",
    "print(f\"MAE: {avg_mae:.4f} ± {std_mae:.4f}\")\n",
    "print(f\"R²: {avg_r2:.4f} ± {std_r2:.4f}\")\n",
    "\n",
    "plot_active_learning_results(all_results, problem='regression', saveas='concrete_strength_uncertainty_sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b87ca",
   "metadata": {},
   "source": [
    "#### Abalone Age Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda4f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c0686",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = (int)(0.8 * len(X_train6))\n",
    "n_samples = max_samples // 25\n",
    "budget = max_samples // n_samples\n",
    "\n",
    "# Define the results file path\n",
    "results_file = '../data/abalone_age_uncertainty_results.pkl'\n",
    "\n",
    "# Check if results already exist\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    \n",
    "    all_results = saved_data['all_results']\n",
    "    final_performances = saved_data['final_performances']\n",
    "else:\n",
    "    all_results = []\n",
    "    final_performances = []\n",
    "    \n",
    "    # Perform ten independent runs\n",
    "    for i in range(10):\n",
    "        np.random.seed(42 + i)\n",
    "        torch.manual_seed(42 + i)\n",
    "        # Abalone Age Active Learning\n",
    "        active_results, final_model = uncertainty_sampling(\n",
    "            (X_train6, X_val6, X_test6),\n",
    "            (y_train6, y_val6, y_test6),\n",
    "            problem=\"regression\",\n",
    "            budget=budget,\n",
    "            n_samples=n_samples,\n",
    "            hidden_units=16,\n",
    "            n_epochs=500,\n",
    "            batch_size=16,\n",
    "            learning_rate=0.05,\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-04,\n",
    "        )\n",
    "        all_results.append(active_results)\n",
    "        \n",
    "        # Store final performance for this run\n",
    "        final_result = active_results[-1]\n",
    "        final_performances.append({\n",
    "            'mse': final_result['mse'],\n",
    "            'root_mse': final_result['root_mse'],\n",
    "            'mae': final_result['mae'],\n",
    "            'r2_score': final_result['r2_score'],\n",
    "            'training_size': final_result['training_size']\n",
    "        })\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'all_results': all_results,\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'max_samples': max_samples,\n",
    "                'n_samples': n_samples,\n",
    "                'budget': budget,\n",
    "                'hidden_units': 16,\n",
    "                'n_epochs': 500,\n",
    "                'batch_size': 16,\n",
    "                'learning_rate': 0.05,\n",
    "                'momentum': 0.9,\n",
    "                'weight_decay': 1e-04\n",
    "            }\n",
    "        }, f)\n",
    "\n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE FINAL PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_mse = np.mean([p['mse'] for p in final_performances])\n",
    "std_mse = np.std([p['mse'] for p in final_performances])\n",
    "avg_root_mse = np.mean([p['root_mse'] for p in final_performances])\n",
    "std_root_mse = np.std([p['root_mse'] for p in final_performances])\n",
    "avg_mae = np.mean([p['mae'] for p in final_performances])\n",
    "std_mae = np.std([p['mae'] for p in final_performances])\n",
    "avg_r2 = np.mean([p['r2_score'] for p in final_performances])\n",
    "std_r2 = np.std([p['r2_score'] for p in final_performances])\n",
    "print(f\"MSE: {avg_mse:.4f} ± {std_mse:.4f}\")\n",
    "print(f\"Root MSE: {avg_root_mse:.4f} ± {std_root_mse:.4f}\")\n",
    "print(f\"MAE: {avg_mae:.4f} ± {std_mae:.4f}\")\n",
    "print(f\"R²: {avg_r2:.4f} ± {std_r2:.4f}\")\n",
    "\n",
    "plot_active_learning_results(all_results, problem='regression', saveas='abalone_age_uncertainty_sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c15da",
   "metadata": {},
   "source": [
    "## Active Learning: Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57517bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7960946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_informative_patterns(model, X_candidate, y_candidate, beta=0.9):\n",
    "    \"\"\"\n",
    "    Vectorized implementation of SASLA pattern selection for improved performance.\n",
    "    \"\"\"\n",
    "    # Extract weights and biases\n",
    "    params = list(model.parameters())\n",
    "    if len(params) < 4:\n",
    "        return X_candidate, y_candidate, {}\n",
    "    \n",
    "    v_weights = params[0]\n",
    "    v_bias = params[1]\n",
    "    w_weights = params[2]\n",
    "    w_bias = params[3]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden_pre = torch.matmul(X_candidate, v_weights.T) + v_bias \n",
    "        hidden_post = torch.sigmoid(hidden_pre)  # y_j\n",
    "        \n",
    "        output_pre = torch.matmul(hidden_post, w_weights.T) + w_bias \n",
    "        output_probs = torch.sigmoid(output_pre)  # o_k \n",
    "        \n",
    "        # Compute v_ji * x_i for all patterns and hidden units\n",
    "        v_ji_xi = torch.matmul(X_candidate, v_weights.T)\n",
    "        \n",
    "        hidden_expanded = hidden_post.unsqueeze(1)\n",
    "        w_weights_expanded = w_weights.unsqueeze(0)\n",
    "        v_ji_xi_expanded = v_ji_xi.unsqueeze(1)\n",
    "        \n",
    "        # computation of w_kj * (1 - y_j) * v_ji * x_i\n",
    "        # Sum over hidden units j for each pattern and output class\n",
    "        sensitivity_sum = torch.sum(\n",
    "            w_weights_expanded * (1 - hidden_expanded) * v_ji_xi_expanded, \n",
    "            dim=2\n",
    "        ) \n",
    "        \n",
    "        # Apply output derivative: (1-o_k)*o_k\n",
    "        output_derivative = output_probs * (1 - output_probs)\n",
    "        \n",
    "        # Complete sensitivity computation\n",
    "        class_sensitivities = torch.abs(output_derivative * sensitivity_sum)\n",
    "        \n",
    "        # Pattern informativeness sum across all output classes\n",
    "        informativeness_scores = torch.sum(class_sensitivities, dim=1)\n",
    "        \n",
    "        # Convert to numpy for threshold computation\n",
    "        informativeness_np = informativeness_scores.cpu().numpy()\n",
    "    \n",
    "    # Calculate selection threshold\n",
    "    avg_informativeness = np.mean(informativeness_np)\n",
    "    selection_threshold = (1 - beta) * avg_informativeness\n",
    "    \n",
    "    # Select patterns above threshold\n",
    "    selected_mask = informativeness_np > selection_threshold\n",
    "    \n",
    "    # Ensure at least one pattern is selected fallback\n",
    "    if not np.any(selected_mask):\n",
    "        most_informative_idx = np.argmax(informativeness_np)\n",
    "        selected_mask[most_informative_idx] = True\n",
    "    \n",
    "    selected_X = X_candidate[selected_mask]\n",
    "    selected_y = y_candidate[selected_mask]\n",
    "    \n",
    "    selection_stats = {\n",
    "        'n_selected': selected_mask.sum(),\n",
    "        'n_total': len(X_candidate),\n",
    "        'selection_ratio': selected_mask.sum() / len(X_candidate),\n",
    "        'avg_informativeness': avg_informativeness,\n",
    "        'threshold': selection_threshold,\n",
    "        'selected_informativeness': informativeness_np[selected_mask].mean() if np.any(selected_mask) else 0.0,\n",
    "        'min_informativeness': informativeness_np.min(),\n",
    "        'max_informativeness': informativeness_np.max()\n",
    "    }\n",
    "\n",
    "    return selected_X, selected_y, selection_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac925101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_sasla(model, X_train, y_train, X_val, y_val, criterion=None, optimizer=None, \n",
    "                     n_epochs=100, batch_size=32, beta=0.9, mse_threshold=1e-4, v=True):\n",
    "    \"\"\"\n",
    "    Train neural network using SASLA - modified to select new subset after each epoch\n",
    "    \"\"\"\n",
    "    if criterion is None:\n",
    "        criterion = nn.MSELoss()\n",
    "    if optimizer is None:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Step 1: Initialize\n",
    "    DC = X_train.clone()  # Candidate set DC\n",
    "    DC_labels = y_train.clone()\n",
    "    \n",
    "    # Initial subset DS0 = DC\n",
    "    DS_current = DC.clone()\n",
    "    DS_current_labels = DC_labels.clone()\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], \n",
    "        'val_loss': [], \n",
    "        'selection_stats': [],\n",
    "        'subset_sizes': [],\n",
    "        'train_accuracy': [],\n",
    "        'subset_intervals': []\n",
    "    }\n",
    "    \n",
    "    total_epochs = 0\n",
    "    converged = False\n",
    "    \n",
    "    # Step 2: Repeat until convergence\n",
    "    while not converged and total_epochs < n_epochs:\n",
    "        if v:\n",
    "            print(f\"\\n--- Epoch {total_epochs + 1} ---\")\n",
    "            print(f\"Training subset size: {len(DS_current)}\")\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        permutation = torch.randperm(DS_current.size()[0])\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = (DS_current.size()[0] + batch_size - 1) // batch_size\n",
    "        \n",
    "        for i in range(0, DS_current.size()[0], batch_size):\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_X, batch_y = DS_current[indices], DS_current_labels[indices]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / num_batches\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val).item()\n",
    "        \n",
    "        # Store metrics\n",
    "        history['train_loss'].append(avg_epoch_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['subset_sizes'].append(len(DS_current))\n",
    "        \n",
    "        if v:\n",
    "            status = f\"Epoch {total_epochs+1}: \"\n",
    "            status += f\"Loss={avg_epoch_loss:.4f}, Val Loss={val_loss:.4f}\"\n",
    "            print(status)\n",
    "        \n",
    "        # Check termination criteria\n",
    "        if avg_epoch_loss < mse_threshold:\n",
    "            if v:\n",
    "                print(f\"Termination: MSE {avg_epoch_loss:.6f} below threshold {mse_threshold}\")\n",
    "            converged = True\n",
    "        \n",
    "        total_epochs += 1\n",
    "        \n",
    "        # Select new subset after each epoch\n",
    "        if not converged and total_epochs < n_epochs:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Step 2b: Compute new training subset DSs\n",
    "                DS_new, DS_new_labels, stats = select_informative_patterns(\n",
    "                    model, DC, DC_labels, beta=beta\n",
    "                )\n",
    "                \n",
    "                # Store selection statistics\n",
    "                stats['epoch'] = total_epochs\n",
    "                history['selection_stats'].append(stats)\n",
    "                history['subset_intervals'].append(total_epochs)\n",
    "                \n",
    "                if v:\n",
    "                    print(f\"Selected {stats['n_selected']} out of {stats['n_total']} patterns\")\n",
    "                    print(f\"Average informativeness: {stats['avg_informativeness']:.6f}\")\n",
    "                \n",
    "                # Update current training subset: DT = DSs\n",
    "                DS_current = DS_new\n",
    "                DS_current_labels = DS_new_labels\n",
    "    \n",
    "    if v:\n",
    "        print(f\"\\nSASLA training completed after {total_epochs} epochs\")\n",
    "        print(f\"Final subset size: {len(DS_current)}\")\n",
    "        print(f\"Final training loss: {history['train_loss'][-1]:.6f}\")\n",
    "        print(f\"Final validation loss: {history['val_loss'][-1]:.6f}\")\n",
    "        \n",
    "        # Convergence reason\n",
    "        if total_epochs >= n_epochs:\n",
    "            print(\"Termination reason: Maximum epochs reached\")\n",
    "        elif history['train_loss'][-1] < mse_threshold:\n",
    "            print(\"Termination reason: MSE threshold achieved\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sasla_history(history):\n",
    "    \"\"\"\n",
    "    Plot SASLA-specific training metrics.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "    \n",
    "    # Training and validation loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', alpha=0.7)\n",
    "    axes[0].plot(history['val_loss'], label='Validation Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training Progress')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Subset sizes over iterations\n",
    "    if history['subset_sizes']:\n",
    "        axes[1].plot(history['subset_sizes'], 'o-')\n",
    "        axes[1].set_xlabel('Subset Iteration')\n",
    "        axes[1].set_ylabel('Selected Patterns')\n",
    "        axes[1].set_title('Pattern Selection Over Time')\n",
    "        axes[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e10306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sasla_selection_plot(history, saveas=None):\n",
    "    fig, ax = plt.subplots(figsize=(column_width_inches, column_width_inches * 0.75))\n",
    "    \n",
    "    ax.plot(history['subset_sizes'], 'o-')\n",
    "    ax.set_xlabel('Subset Iteration', fontsize=16)\n",
    "    ax.set_ylabel('Selected Patterns', fontsize=16)\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if saveas:\n",
    "        plt.savefig(f\"../report/plots/{saveas}.pdf\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991de5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_pipeline_sasla(X, y, problem='classification', hidden_units=10, n_epochs=100, batch_size=32,\n",
    "                           learning_rate=0.001, momentum=0.9, weight_decay=1e-5, \n",
    "                           beta=0.9, v=True):\n",
    "    \"\"\"\n",
    "    SASLA training pipeline for classification and regression tasks.\n",
    "    \"\"\"\n",
    "    # unzip data\n",
    "    X_train, X_val, X_test = X\n",
    "    y_train, y_val, y_test = y\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    if problem == \"classification\":\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        n_output = y_train.shape[1]\n",
    "    else:\n",
    "        y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)\n",
    "        y_val_tensor = torch.FloatTensor(y_val).view(-1, 1)\n",
    "        n_output = 1\n",
    "  \n",
    "    n_input = X_train.shape[1]\n",
    "    n_hidden = hidden_units\n",
    "\n",
    "    model = NeuralNetwork(n_input, n_hidden, n_output)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    \n",
    "    # Train using SASLA\n",
    "    trained_model, history = train_model_sasla(\n",
    "        model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor,\n",
    "        criterion=criterion, optimizer=optimizer,\n",
    "        n_epochs=n_epochs, batch_size=batch_size, beta=beta, v=v\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    results = evaluate_model(trained_model, X_test, y_test, problem=problem, v=v)\n",
    "    \n",
    "    if v:\n",
    "        # Plot training curves\n",
    "        plot_sasla_history(history)\n",
    "    \n",
    "    return trained_model, history, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7facb8",
   "metadata": {},
   "source": [
    "#### Breast Cancer Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c6e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast cancer\n",
    "X = (X_train1, X_val1, X_test1)\n",
    "y = (y_train1, y_val1, y_test1)\n",
    "\n",
    "results_file = '../data/breast_cancer_sasla_results.pkl'\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    final_performances = saved_data['final_performances']\n",
    "    model, history, results = training_pipeline_sasla(X, y, hidden_units=16, n_epochs=500, batch_size=32, learning_rate=0.1, momentum=0.8, weight_decay=0.0001, v=True)\n",
    "    create_sasla_selection_plot(history, saveas='breast_cancer_sasla_selection')\n",
    "else:\n",
    "    final_performances = []\n",
    "    for i in tqdm.tqdm(range(9), desc=\"Running multiple independent runs:\"):\n",
    "        model, history, results = training_pipeline_sasla(X, y, hidden_units=16, n_epochs=500, batch_size=32, learning_rate=0.1, momentum=0.8, weight_decay=0.0001, v=False)\n",
    "        final_performances.append(results)\n",
    "\n",
    "    model, history, results = training_pipeline_sasla(X, y, hidden_units=16, n_epochs=500, batch_size=32, learning_rate=0.1, momentum=0.8, weight_decay=0.0001, v=True)\n",
    "    final_performances.append(results)\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'hidden_units': 16,\n",
    "                'n_epochs': 500,\n",
    "                'batch_size': 32,\n",
    "                'learning_rate': 0.1,\n",
    "                'momentum': 0.8,\n",
    "                'weight_decay': 0.0001\n",
    "            }\n",
    "        }, f)\n",
    "        \n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE SASLA PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_accuracy = np.mean([p['accuracy'] for p in final_performances])\n",
    "std_accuracy = np.std([p['accuracy'] for p in final_performances])\n",
    "avg_f1 = np.mean([p['f1_score'] for p in final_performances])\n",
    "std_f1 = np.std([p['f1_score'] for p in final_performances])\n",
    "avg_precision = np.mean([p['precision'] for p in final_performances])\n",
    "std_precision = np.std([p['precision'] for p in final_performances])\n",
    "avg_recall = np.mean([p['recall'] for p in final_performances])\n",
    "std_recall = np.std([p['recall'] for p in final_performances])\n",
    "avg_mcc = np.mean([p['mcc'] for p in final_performances])\n",
    "std_mcc = np.std([p['mcc'] for p in final_performances])\n",
    "\n",
    "print(f\"Accuracy:  {avg_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "print(f\"F1 Score:  {avg_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Precision: {avg_precision:.4f} ± {std_precision:.4f}\")\n",
    "print(f\"Recall:    {avg_recall:.4f} ± {std_recall:.4f}\")\n",
    "print(f\"MCC:       {avg_mcc:.4f} ± {std_mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3ae2e7",
   "metadata": {},
   "source": [
    "#### Mobile Price Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3534c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mobile dataset\n",
    "X = (X_train2, X_val2, X_test2)\n",
    "y = (y_train2, y_val2, y_test2)\n",
    "\n",
    "results_file = '../data/mobile_pricing_sasla_results.pkl'\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    final_performances = saved_data['final_performances']\n",
    "    model, history, results = training_pipeline_sasla(X, y, hidden_units=12, n_epochs=200, batch_size=16, learning_rate=0.1, momentum=0.9, weight_decay=1e-05)\n",
    "    create_sasla_selection_plot(history, saveas='mobile_pricing_sasla_selection')\n",
    "else:\n",
    "    final_performances = []\n",
    "    for i in tqdm.tqdm(range(9), desc=\"Running multiple independent runs:\"):\n",
    "        model, history, results = training_pipeline_sasla(X, y, hidden_units=12, n_epochs=200, batch_size=16, learning_rate=0.1, momentum=0.9, weight_decay=1e-05, v=False)\n",
    "        final_performances.append(results)\n",
    "\n",
    "    model, history, results = training_pipeline_sasla(X, y, hidden_units=12, n_epochs=200, batch_size=16, learning_rate=0.1, momentum=0.9, weight_decay=1e-05)\n",
    "    final_performances.append(results)\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'hidden_units': 12,\n",
    "                'n_epochs': 200,\n",
    "                'batch_size': 16,\n",
    "                'learning_rate': 0.1,\n",
    "                'momentum': 0.9,\n",
    "                'weight_decay': 0.00001\n",
    "            }\n",
    "        }, f)\n",
    "        \n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE SASLA PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_accuracy = np.mean([p['accuracy'] for p in final_performances])\n",
    "std_accuracy = np.std([p['accuracy'] for p in final_performances])\n",
    "avg_f1 = np.mean([p['f1_score'] for p in final_performances])\n",
    "std_f1 = np.std([p['f1_score'] for p in final_performances])\n",
    "avg_precision = np.mean([p['precision'] for p in final_performances])\n",
    "std_precision = np.std([p['precision'] for p in final_performances])\n",
    "avg_recall = np.mean([p['recall'] for p in final_performances])\n",
    "std_recall = np.std([p['recall'] for p in final_performances])\n",
    "avg_mcc = np.mean([p['mcc'] for p in final_performances])\n",
    "std_mcc = np.std([p['mcc'] for p in final_performances])\n",
    "\n",
    "print(f\"Accuracy:  {avg_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "print(f\"F1 Score:  {avg_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Precision: {avg_precision:.4f} ± {std_precision:.4f}\")\n",
    "print(f\"Recall:    {avg_recall:.4f} ± {std_recall:.4f}\")\n",
    "print(f\"MCC:       {avg_mcc:.4f} ± {std_mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5cdc00",
   "metadata": {},
   "source": [
    "#### Letter Recognition Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letter recognition\n",
    "X = (X_train3, X_val3, X_test3)\n",
    "y = (y_train3, y_val3, y_test3)\n",
    "\n",
    "results_file = '../data/letter_recognition_sasla_results.pkl'\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    final_performances = saved_data['final_performances']\n",
    "    model, history, results = training_pipeline_sasla(X, y, hidden_units=24, n_epochs=1000, batch_size=32, learning_rate=0.5, momentum=0.9, weight_decay=1e-05)\n",
    "    create_sasla_selection_plot(history, saveas='letter_recognition_sasla_selection')\n",
    "else:\n",
    "    final_performances = []\n",
    "    for i in tqdm.tqdm(range(9), desc=\"Running multiple independent runs:\"):\n",
    "        model, history, results = training_pipeline_sasla(X, y, hidden_units=24, n_epochs=1000, batch_size=32, learning_rate=0.5, momentum=0.9, weight_decay=1e-05, v=False)\n",
    "        final_performances.append(results)\n",
    "\n",
    "    model, history, results = training_pipeline_sasla(X, y, hidden_units=24, n_epochs=1000, batch_size=32, learning_rate=0.5, momentum=0.9, weight_decay=1e-05)\n",
    "    final_performances.append(results)\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'hidden_units': 24,\n",
    "                'n_epochs': 1000,\n",
    "                'batch_size': 32,\n",
    "                'learning_rate': 0.5,\n",
    "                'momentum': 0.9,\n",
    "                'weight_decay': 0.00001\n",
    "            }\n",
    "        }, f)\n",
    "        \n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE SASLA PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_accuracy = np.mean([p['accuracy'] for p in final_performances])\n",
    "std_accuracy = np.std([p['accuracy'] for p in final_performances])\n",
    "avg_f1 = np.mean([p['f1_score'] for p in final_performances])\n",
    "std_f1 = np.std([p['f1_score'] for p in final_performances])\n",
    "avg_precision = np.mean([p['precision'] for p in final_performances])\n",
    "std_precision = np.std([p['precision'] for p in final_performances])\n",
    "avg_recall = np.mean([p['recall'] for p in final_performances])\n",
    "std_recall = np.std([p['recall'] for p in final_performances])\n",
    "avg_mcc = np.mean([p['mcc'] for p in final_performances])\n",
    "std_mcc = np.std([p['mcc'] for p in final_performances])\n",
    "\n",
    "print(f\"Accuracy:  {avg_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "print(f\"F1 Score:  {avg_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Precision: {avg_precision:.4f} ± {std_precision:.4f}\")\n",
    "print(f\"Recall:    {avg_recall:.4f} ± {std_recall:.4f}\")\n",
    "print(f\"MCC:       {avg_mcc:.4f} ± {std_mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e363a7",
   "metadata": {},
   "source": [
    "#### Boston Housing Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf74ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housing prices\n",
    "X = (X_train4, X_val4, X_test4)\n",
    "y = (y_train4, y_val4, y_test4)\n",
    "\n",
    "results_file = '../data/boston_housing_sasla_results.pkl'\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    final_performances = saved_data['final_performances']\n",
    "    model, history, results = training_pipeline_sasla(X, y, problem=\"regression\", hidden_units=12, n_epochs=500, batch_size=16, learning_rate=0.5, momentum=0.8, weight_decay=1e-05)\n",
    "    create_sasla_selection_plot(history, saveas='boston_housing_sasla_selection')\n",
    "else:\n",
    "    final_performances = []\n",
    "    for i in tqdm.tqdm(range(9), desc=\"Running multiple independent runs:\"):\n",
    "        model, history, results = training_pipeline_sasla(X, y, problem=\"regression\", hidden_units=12, n_epochs=500, batch_size=16, learning_rate=0.5, momentum=0.8, weight_decay=1e-05, v=False)\n",
    "        final_performances.append(results)\n",
    "\n",
    "    model, history, results = training_pipeline_sasla(X, y, problem=\"regression\", hidden_units=12, n_epochs=500, batch_size=16, learning_rate=0.5, momentum=0.8, weight_decay=1e-05)\n",
    "    final_performances.append(results)\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'hidden_units': 12,\n",
    "                'n_epochs': 500,\n",
    "                'batch_size': 16,\n",
    "                'learning_rate': 0.5,\n",
    "                'momentum': 0.8,\n",
    "                'weight_decay': 0.00001\n",
    "            }\n",
    "        }, f)\n",
    "        \n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE SASLA PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_mse = np.mean([p['mse'] for p in final_performances])\n",
    "std_mse = np.std([p['mse'] for p in final_performances])\n",
    "avg_root_mse = np.mean([p['root_mse'] for p in final_performances])\n",
    "std_root_mse = np.std([p['root_mse'] for p in final_performances])\n",
    "avg_mae = np.mean([p['mae'] for p in final_performances])\n",
    "std_mae = np.std([p['mae'] for p in final_performances])\n",
    "avg_r2 = np.mean([p['r2_score'] for p in final_performances])\n",
    "std_r2 = np.std([p['r2_score'] for p in final_performances])\n",
    "\n",
    "print(f\"MSE: {avg_mse:.4f} ± {std_mse:.4f}\")\n",
    "print(f\"Root MSE: {avg_root_mse:.4f} ± {std_root_mse:.4f}\")\n",
    "print(f\"MAE: {avg_mae:.4f} ± {std_mae:.4f}\")\n",
    "print(f\"R²: {avg_r2:.4f} ± {std_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3793efb8",
   "metadata": {},
   "source": [
    "#### Concrete Strength Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26670591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concrete dataset\n",
    "X = (X_train5, X_val5, X_test5)\n",
    "y = (y_train5, y_val5, y_test5)\n",
    "\n",
    "results_file = '../data/concrete_strength_sasla_results.pkl'\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    final_performances = saved_data['final_performances']\n",
    "    model, history, results = training_pipeline_sasla(X, y, problem=\"regression\", hidden_units=8, n_epochs=500, batch_size=16, learning_rate=0.5, momentum=0.8, weight_decay=1e-05)\n",
    "    create_sasla_selection_plot(history, saveas='concrete_strength_sasla_selection')\n",
    "else:\n",
    "    final_performances = []\n",
    "    for i in tqdm.tqdm(range(9), desc=\"Running multiple independent runs:\"):\n",
    "        model, history, results = training_pipeline_sasla(X, y, problem=\"regression\", hidden_units=8, n_epochs=500, batch_size=16, learning_rate=0.5, momentum=0.8, weight_decay=1e-05, v=False)\n",
    "        final_performances.append(results)\n",
    "\n",
    "    model, history, results = training_pipeline_sasla(X, y, problem=\"regression\", hidden_units=8, n_epochs=500, batch_size=16, learning_rate=0.5, momentum=0.8, weight_decay=1e-05)\n",
    "    final_performances.append(results)\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'hidden_units': 8,\n",
    "                'n_epochs': 500,\n",
    "                'batch_size': 16,\n",
    "                'learning_rate': 0.5,\n",
    "                'momentum': 0.8,\n",
    "                'weight_decay': 0.00001\n",
    "            }\n",
    "        }, f)\n",
    "        \n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE SASLA PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_mse = np.mean([p['mse'] for p in final_performances])\n",
    "std_mse = np.std([p['mse'] for p in final_performances])\n",
    "avg_root_mse = np.mean([p['root_mse'] for p in final_performances])\n",
    "std_root_mse = np.std([p['root_mse'] for p in final_performances])\n",
    "avg_mae = np.mean([p['mae'] for p in final_performances])\n",
    "std_mae = np.std([p['mae'] for p in final_performances])\n",
    "avg_r2 = np.mean([p['r2_score'] for p in final_performances])\n",
    "std_r2 = np.std([p['r2_score'] for p in final_performances])\n",
    "\n",
    "print(f\"MSE: {avg_mse:.4f} ± {std_mse:.4f}\")\n",
    "print(f\"Root MSE: {avg_root_mse:.4f} ± {std_root_mse:.4f}\")\n",
    "print(f\"MAE: {avg_mae:.4f} ± {std_mae:.4f}\")\n",
    "print(f\"R²: {avg_r2:.4f} ± {std_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70caee9",
   "metadata": {},
   "source": [
    "#### Abalone Age Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359fd283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abalone dataset\n",
    "X = (X_train6, X_val6, X_test6)\n",
    "y = (y_train6, y_val6, y_test6)\n",
    "\n",
    "results_file = '../data/abalone_age_sasla_results.pkl'\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    final_performances = saved_data['final_performances']\n",
    "    model, history, results = training_pipeline_sasla(X, y, problem=\"regression\", hidden_units=24, n_epochs=500, batch_size=16, learning_rate=0.05, momentum=0.9, weight_decay=1e-04)\n",
    "    create_sasla_selection_plot(history, saveas='abalone_age_sasla_selection')\n",
    "else:\n",
    "    final_performances = []\n",
    "    for i in tqdm.tqdm(range(9), desc=\"Running multiple independent runs:\"):\n",
    "        model, history, results = training_pipeline_sasla(X, y, problem=\"regression\", hidden_units=16, n_epochs=500, batch_size=16, learning_rate=0.05, momentum=0.9, weight_decay=1e-04, v=False)\n",
    "        final_performances.append(results)\n",
    "\n",
    "    model, history, results = training_pipeline_sasla(X, y, problem=\"regression\", hidden_units=16, n_epochs=500, batch_size=16, learning_rate=0.05, momentum=0.9, weight_decay=1e-04)\n",
    "    final_performances.append(results)\n",
    "    \n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'final_performances': final_performances,\n",
    "            'experiment_config': {\n",
    "                'hidden_units': 16,\n",
    "                'n_epochs': 500,\n",
    "                'batch_size': 16,\n",
    "                'learning_rate': 0.05,\n",
    "                'momentum': 0.9,\n",
    "                'weight_decay': 0.0001\n",
    "            }\n",
    "        }, f)\n",
    "        \n",
    "# Calculate and print average final performance measures\n",
    "print(\"=\"*60)\n",
    "print(\"AVERAGE SASLA PERFORMANCE ACROSS 10 RUNS\")\n",
    "print(\"=\"*60)\n",
    "avg_mse = np.mean([p['mse'] for p in final_performances])\n",
    "std_mse = np.std([p['mse'] for p in final_performances])\n",
    "avg_root_mse = np.mean([p['root_mse'] for p in final_performances])\n",
    "std_root_mse = np.std([p['root_mse'] for p in final_performances])\n",
    "avg_mae = np.mean([p['mae'] for p in final_performances])\n",
    "std_mae = np.std([p['mae'] for p in final_performances])\n",
    "avg_r2 = np.mean([p['r2_score'] for p in final_performances])\n",
    "std_r2 = np.std([p['r2_score'] for p in final_performances])\n",
    "\n",
    "print(f\"MSE: {avg_mse:.4f} ± {std_mse:.4f}\")\n",
    "print(f\"Root MSE: {avg_root_mse:.4f} ± {std_root_mse:.4f}\")\n",
    "print(f\"MAE: {avg_mae:.4f} ± {std_mae:.4f}\")\n",
    "print(f\"R²: {avg_r2:.4f} ± {std_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e035c66",
   "metadata": {},
   "source": [
    "## Empirical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62940c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_results():\n",
    "    datasets = {\n",
    "        'classification': ['breast_cancer', 'mobile_pricing', 'letter_recognition'],\n",
    "        'regression': ['boston_housing', 'concrete_strength', 'abalone_age']\n",
    "    }\n",
    "    \n",
    "    approaches = ['passive', 'uncertainty', 'sasla']\n",
    "    \n",
    "    all_results = {\n",
    "        'classification': {},\n",
    "        'regression': {}\n",
    "    }\n",
    "    \n",
    "    for problem_type in ['classification', 'regression']:\n",
    "        for dataset in datasets[problem_type]:\n",
    "            all_results[problem_type][dataset] = {}\n",
    "            \n",
    "            for approach in approaches:\n",
    "                filename = f'../data/{dataset}_{approach}_results.pkl'\n",
    "                try:\n",
    "                    with open(filename, 'rb') as f:\n",
    "                        data = pickle.load(f)\n",
    "                    all_results[problem_type][dataset][approach] = data['final_performances']\n",
    "                    print(f\"Loaded: {filename} - {len(data['final_performances'])} runs\")\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"Warning: {filename} not found\")\n",
    "                    all_results[problem_type][dataset][approach] = []\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def extract_performance_scores(all_results, problem_type):\n",
    "    datasets = list(all_results[problem_type].keys())\n",
    "    approaches = ['passive', 'uncertainty', 'sasla']\n",
    "    \n",
    "    scores = {approach: [] for approach in approaches}\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        for approach in approaches:\n",
    "            if problem_type == 'classification':\n",
    "                dataset_scores = [run['f1_score'] for run in all_results[problem_type][dataset][approach]]\n",
    "            else:\n",
    "                dataset_scores = [run['r2_score'] for run in all_results[problem_type][dataset][approach]]\n",
    "            \n",
    "            mean_score = np.mean(dataset_scores) if dataset_scores else 0.0\n",
    "            scores[approach].append(mean_score)\n",
    "    \n",
    "    return scores, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68441d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_boxplots(all_results):\n",
    "    classification_metrics = ['accuracy', 'f1_score', 'precision', 'recall']\n",
    "    regression_metrics = ['r2_score', 'mse', 'mae', 'root_mse']\n",
    "    algorithms = ['passive', 'uncertainty', 'sasla']\n",
    "    algorithm_labels = ['PL', 'US', 'SASLA']\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(text_width_inches, 3))\n",
    "    \n",
    "    # Prepare classification results dictionary\n",
    "    classification_results = {}\n",
    "    datasets_class = ['breast_cancer', 'mobile_pricing', 'letter_recognition']\n",
    "    \n",
    "    for metric in classification_metrics:\n",
    "        classification_results[metric] = {alg: [] for alg in algorithms}\n",
    "        \n",
    "        for dataset in datasets_class:\n",
    "            for alg in algorithms:\n",
    "                results = all_results['classification'][dataset][alg]\n",
    "                if results:\n",
    "                    metric_values = [run[metric] for run in results]\n",
    "                    classification_results[metric][alg].extend(metric_values)\n",
    "    \n",
    "    # Create classification box plots\n",
    "    for i, metric in enumerate(classification_metrics):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        data_to_plot = [classification_results[metric][alg] for alg in algorithms]\n",
    "        box_plot = ax.boxplot(data_to_plot, tick_labels=algorithm_labels, patch_artist=True)\n",
    "        \n",
    "        for median_line in box_plot['medians']:\n",
    "            median_line.set_color('red')\n",
    "            median_line.set_linewidth(2)\n",
    "        \n",
    "        # Set title and labels\n",
    "        metric_name = metric.replace('_', ' ').title()\n",
    "        if metric == 'f1_score':\n",
    "            metric_name = 'F1 Score'\n",
    "        \n",
    "        ax.set_title(f'{metric_name}', fontweight='bold', fontsize=14)\n",
    "        ax.set_ylabel('Score', fontsize=16)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../report/plots/classification_boxplots.pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Regression boxplots\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(text_width_inches, 3))\n",
    "    \n",
    "    # Prepare regression results dictionary\n",
    "    regression_results = {}\n",
    "    datasets_reg = ['boston_housing', 'concrete_strength', 'abalone_age']\n",
    "    \n",
    "    for metric in regression_metrics:\n",
    "        regression_results[metric] = {alg: [] for alg in algorithms}\n",
    "        \n",
    "        for dataset in datasets_reg:\n",
    "            for alg in algorithms:\n",
    "                results = all_results['regression'][dataset][alg]\n",
    "                if results:\n",
    "                    metric_values = [run[metric] for run in results]\n",
    "                    regression_results[metric][alg].extend(metric_values)\n",
    "    \n",
    "    # Create regression box plots\n",
    "    for i, metric in enumerate(regression_metrics):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Prepare data for box plot\n",
    "        data_to_plot = [regression_results[metric][alg] for alg in algorithms]\n",
    "        \n",
    "        # Create box plot\n",
    "        box_plot = ax.boxplot(data_to_plot, tick_labels=algorithm_labels, patch_artist=True)\n",
    "        \n",
    "        for median_line in box_plot['medians']:\n",
    "            median_line.set_color('red')\n",
    "            median_line.set_linewidth(2)\n",
    "        \n",
    "        # Set title and labels\n",
    "        metric_name = metric.replace('_', ' ').title()\n",
    "        if metric == 'r2_score':\n",
    "            metric_name = 'R-squared Score'\n",
    "        elif metric == 'mse':\n",
    "            metric_name = 'MSE'\n",
    "        elif metric == 'mae':\n",
    "            metric_name = 'MAE'\n",
    "        elif metric == 'root_mse':\n",
    "            metric_name = 'Root MSE'\n",
    "        \n",
    "        ax.set_title(f'{metric_name}', fontweight='bold', fontsize=14)\n",
    "        ax.set_ylabel('Score', fontsize=16)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../report/plots/regression_boxplots.pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Execute the analysis\n",
    "print(\"Creating comprehensive box plots...\")\n",
    "all_results = load_all_results()\n",
    "create_comprehensive_boxplots(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e71b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_critical_difference_plot_three_algorithms(scores, algorithm_names, problem_type=\"Classification\", saveas=None, alpha=0.05):\n",
    "    # Convert to numpy arrays\n",
    "    score_arrays = [np.array(scores[name]) for name in algorithm_names]\n",
    "    n_datasets = len(score_arrays[0])\n",
    "    k_algorithms = len(algorithm_names)\n",
    "    \n",
    "    # Calculate ranks for each dataset\n",
    "    ranks_matrix = []\n",
    "    for i in range(n_datasets):\n",
    "        dataset_scores = [score_arrays[j][i] for j in range(k_algorithms)]\n",
    "        # Use negative scores for ranking\n",
    "        dataset_ranks = rankdata([-score for score in dataset_scores])\n",
    "        ranks_matrix.append(dataset_ranks)\n",
    "    \n",
    "    ranks_matrix = np.array(ranks_matrix)\n",
    "    \n",
    "    # Calculate mean ranks\n",
    "    mean_ranks = pd.Series([np.mean(ranks_matrix[:, i]) for i in range(k_algorithms)], \n",
    "                          index=algorithm_names)\n",
    "    \n",
    "    # Perform pairwise Wilcoxon signed-rank tests\n",
    "    p_values = np.ones((k_algorithms, k_algorithms))\n",
    "    for i in range(k_algorithms):\n",
    "        for j in range(i+1, k_algorithms):\n",
    "            try:\n",
    "                stat, p_val = wilcoxon(score_arrays[i], score_arrays[j])\n",
    "                p_values[i, j] = p_val\n",
    "                p_values[j, i] = p_val\n",
    "            except ValueError:\n",
    "                # Handle case where differences are all zero\n",
    "                p_values[i, j] = 1.0\n",
    "                p_values[j, i] = 1.0\n",
    "    \n",
    "    # Create significance matrix\n",
    "    sig_matrix = pd.DataFrame(p_values, index=algorithm_names, columns=algorithm_names)\n",
    "    \n",
    "    # Calculate critical difference for Nemenyi test\n",
    "    q_alpha = 2.344 if alpha == 0.05 else 2.569  # For k=3 algorithms\n",
    "    critical_difference = q_alpha * np.sqrt(k_algorithms * (k_algorithms + 1) / (6 * n_datasets))\n",
    "    \n",
    "    # Create the critical difference plot\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.size\": 10,\n",
    "        \"axes.labelsize\": 10,\n",
    "        \"axes.titlesize\": 11,\n",
    "        \"legend.fontsize\": 9,\n",
    "        \"savefig.bbox\": \"tight\",\n",
    "        \"savefig.format\": \"pdf\",\n",
    "        \"savefig.dpi\": 300\n",
    "    })\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(column_width_inches*1.4, 3))\n",
    "    \n",
    "    sp.critical_difference_diagram(\n",
    "        ranks=mean_ranks,\n",
    "        sig_matrix=sig_matrix,\n",
    "        ax=ax,\n",
    "        label_fmt_left='{label}\\n({rank:.2f})',\n",
    "        label_fmt_right='({rank:.2f})\\n{label}',\n",
    "        color_palette=['black', 'black', 'black'],\n",
    "        label_props={'fontsize': 10, 'fontweight': 'bold'},\n",
    "        text_h_margin=0.15\n",
    "    )\n",
    "    \n",
    "    # Add title with critical difference\n",
    "    metric = \"F1 Score\" if problem_type == \"Classification\" else \"R-Squared Score\"\n",
    "    ax.set_title(f'{problem_type} Performance Comparison ({metric})\\nCritical Difference = {critical_difference:.3f}',\n",
    "                fontsize=11, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if saveas:\n",
    "        plt.savefig(f'../report/plots/{saveas}.pdf', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return mean_ranks, sig_matrix, critical_difference\n",
    "\n",
    "def perform_pairwise_analysis(scores, algorithm_names, problem_type):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DETAILED STATISTICAL ANALYSIS - {problem_type.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    score_arrays = [np.array(scores[name]) for name in algorithm_names]\n",
    "    metric = \"F1 Score\" if problem_type == \"Classification\" else \"R² Score\"\n",
    "    \n",
    "    # Print descriptive statistics\n",
    "    print(f\"\\nDescriptive Statistics ({metric}):\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, name in enumerate(algorithm_names):\n",
    "        mean_score = np.mean(score_arrays[i])\n",
    "        std_score = np.std(score_arrays[i])\n",
    "        print(f\"{name:20}: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "    \n",
    "    # Pairwise comparisons\n",
    "    print(f\"\\nPairwise Wilcoxon Signed-Rank Tests:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i in range(len(algorithm_names)):\n",
    "        for j in range(i+1, len(algorithm_names)):\n",
    "            try:\n",
    "                stat, p_val = wilcoxon(score_arrays[i], score_arrays[j])\n",
    "                effect_size = np.mean(score_arrays[i]) - np.mean(score_arrays[j])\n",
    "                \n",
    "                print(f\"{algorithm_names[i]} vs {algorithm_names[j]}:\")\n",
    "                print(f\"  p-value: {p_val:.6f}\")\n",
    "                print(f\"  Effect size: {effect_size:+.4f}\")\n",
    "                print(f\"  Significant: {'Yes' if p_val < 0.05 else 'No'}\")\n",
    "                print()\n",
    "            except ValueError as e:\n",
    "                print(f\"{algorithm_names[i]} vs {algorithm_names[j]}: Error - {e}\")\n",
    "\n",
    "def create_performance_summary_table(all_results):\n",
    "    datasets_class = ['breast_cancer', 'mobile_pricing', 'letter_recognition']\n",
    "    datasets_reg = ['boston_housing', 'concrete_strength', 'abalone_age']\n",
    "    approaches = ['passive', 'uncertainty', 'sasla']\n",
    "    \n",
    "    for dataset in datasets_class:\n",
    "        row = f\"{dataset:<20}\"\n",
    "        for approach in approaches:\n",
    "            results = all_results['classification'][dataset][approach]\n",
    "            if results:\n",
    "                mean_f1 = np.mean([r['f1_score'] for r in results])\n",
    "                std_f1 = np.std([r['f1_score'] for r in results])\n",
    "                row += f\"{mean_f1:.3f}±{std_f1:.3f}   \"\n",
    "            else:\n",
    "                row += f\"{'N/A':<15}\"\n",
    "        print(row)\n",
    "    \n",
    "    for dataset in datasets_reg:\n",
    "        row = f\"{dataset:<20}\"\n",
    "        for approach in approaches:\n",
    "            results = all_results['regression'][dataset][approach]\n",
    "            if results:\n",
    "                mean_r2 = np.mean([r['r2_score'] for r in results])\n",
    "                std_r2 = np.std([r['r2_score'] for r in results])\n",
    "                row += f\"{mean_r2:.3f}±{std_r2:.3f}   \"\n",
    "            else:\n",
    "                row += f\"{'N/A':<15}\"\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b27ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = load_all_results()\n",
    "create_performance_summary_table(all_results)\n",
    "\n",
    "class_scores, class_datasets = extract_performance_scores(all_results, 'classification')\n",
    "algorithm_names = ['passive', 'uncertainty', 'sasla']\n",
    "# Perform statistcal analysis\n",
    "perform_pairwise_analysis(class_scores, algorithm_names, 'Classification')\n",
    "# Create critical difference plot for classification\n",
    "class_mean_ranks, class_sig_matrix, class_cd = create_critical_difference_plot_three_algorithms(\n",
    "    class_scores, algorithm_names, 'Classification', 'classification_critical_difference'\n",
    ")\n",
    "\n",
    "reg_scores, reg_datasets = extract_performance_scores(all_results, 'regression')\n",
    "# Perform statistical analysis\n",
    "perform_pairwise_analysis(reg_scores, algorithm_names, 'Regression')\n",
    "# Create critical difference plot for regression\n",
    "reg_mean_ranks, reg_sig_matrix, reg_cd = create_critical_difference_plot_three_algorithms(\n",
    "    reg_scores, algorithm_names, 'Regression', 'regression_critical_difference'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f420a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def friedman_nemenyi_test(all_results, problem_type='classification', alpha=0.05):\n",
    "    \n",
    "    # Extract performance scores\n",
    "    datasets = list(all_results[problem_type].keys())\n",
    "    algorithms = ['passive', 'uncertainty', 'sasla']\n",
    "    \n",
    "    # Create performance matrix: [n_datasets x n_algorithms]\n",
    "    performance_matrix = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_scores = []\n",
    "        for algorithm in algorithms:\n",
    "            results = all_results[problem_type][dataset][algorithm]\n",
    "            if problem_type == 'classification':\n",
    "                mean_score = np.mean([r['f1_score'] for r in results])\n",
    "            else:\n",
    "                mean_score = np.mean([r['r2_score'] for r in results])\n",
    "            dataset_scores.append(mean_score)\n",
    "        performance_matrix.append(dataset_scores)\n",
    "    \n",
    "    performance_matrix = np.array(performance_matrix)\n",
    "    n_datasets, n_algorithms = performance_matrix.shape\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FRIEDMAN TEST - {problem_type.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Display performance matrix\n",
    "    df_performance = pd.DataFrame(performance_matrix, \n",
    "                                 columns=algorithms, \n",
    "                                 index=datasets)\n",
    "    print(f\"\\nPerformance Matrix:\")\n",
    "    print(df_performance.round(4))\n",
    "    \n",
    "    # Calculate ranks for each dataset (1 = best, 3 = worst)\n",
    "    # For performance metrics, higher is better, so we rank in descending order\n",
    "    ranks_matrix = np.zeros_like(performance_matrix)\n",
    "    for i in range(n_datasets):\n",
    "        # Use negative scores to rank (so highest score gets rank 1)\n",
    "        ranks_matrix[i] = rankdata(-performance_matrix[i])\n",
    "    \n",
    "    print(f\"\\nRanks Matrix (1=best, {n_algorithms}=worst):\")\n",
    "    df_ranks = pd.DataFrame(ranks_matrix, \n",
    "                           columns=algorithms, \n",
    "                           index=datasets)\n",
    "    print(df_ranks)\n",
    "    \n",
    "    # Calculate mean ranks\n",
    "    mean_ranks = np.mean(ranks_matrix, axis=0)\n",
    "    print(f\"\\nMean Ranks:\")\n",
    "    for i, alg in enumerate(algorithms):\n",
    "        print(f\"  {alg}: {mean_ranks[i]:.3f}\")\n",
    "    \n",
    "    # Perform Friedman test\n",
    "    # Prepare data for scipy friedmanchisquare (each algorithm's ranks across datasets)\n",
    "    alg_ranks = [ranks_matrix[:, i] for i in range(n_algorithms)]\n",
    "    \n",
    "    friedman_stat, friedman_p = friedmanchisquare(*alg_ranks)\n",
    "    \n",
    "    print(f\"\\nFriedman Test Results:\")\n",
    "    print(f\"  Chi-square statistic: {friedman_stat:.4f}\")\n",
    "    print(f\"  p-value: {friedman_p:.6f}\")\n",
    "    print(f\"  Significant difference: {'Yes' if friedman_p < alpha else 'No'}\")\n",
    "    \n",
    "    # If Friedman test is significant, perform Nemenyi post-hoc test\n",
    "    nemenyi_results = None\n",
    "    critical_difference = None\n",
    "    \n",
    "    if friedman_p < alpha:\n",
    "        \n",
    "        # Calculate critical difference for Nemenyi test\n",
    "        # q_alpha values for k=3 algorithms at alpha=0.05 and 0.10\n",
    "        q_alpha = 2.344 if alpha == 0.05 else 2.052  # Two-tailed critical values\n",
    "        critical_difference = q_alpha * np.sqrt(n_algorithms * (n_algorithms + 1) / (6 * n_datasets))\n",
    "        \n",
    "        print(f\"Critical Difference (CD): {critical_difference:.4f}\")\n",
    "        print(f\"Alpha level: {alpha}\")\n",
    "        \n",
    "        # Perform pairwise comparisons\n",
    "        nemenyi_results = {}\n",
    "        print(f\"\\nPairwise Comparisons:\")\n",
    "        \n",
    "        for i in range(n_algorithms):\n",
    "            for j in range(i + 1, n_algorithms):\n",
    "                rank_diff = abs(mean_ranks[i] - mean_ranks[j])\n",
    "                is_significant = rank_diff > critical_difference\n",
    "                \n",
    "                better_alg = algorithms[i] if mean_ranks[i] < mean_ranks[j] else algorithms[j]\n",
    "                worse_alg = algorithms[j] if mean_ranks[i] < mean_ranks[j] else algorithms[i]\n",
    "                \n",
    "                nemenyi_results[f\"{algorithms[i]}_vs_{algorithms[j]}\"] = {\n",
    "                    'rank_difference': rank_diff,\n",
    "                    'critical_difference': critical_difference,\n",
    "                    'significant': is_significant,\n",
    "                    'better_algorithm': better_alg if is_significant else 'No significant difference'\n",
    "                }\n",
    "                \n",
    "                print(f\"  {algorithms[i]} vs {algorithms[j]}:\")\n",
    "                print(f\"    Rank difference: {rank_diff:.4f}\")\n",
    "                print(f\"    Significant: {'Yes' if is_significant else 'No'}\")\n",
    "                if is_significant:\n",
    "                    print(f\"    Better algorithm: {better_alg}\")\n",
    "                print()\n",
    "        \n",
    "        # Summary of significant differences\n",
    "        print(f\"Summary of Significant Differences (α = {alpha}):\")\n",
    "        significant_pairs = [(k, v) for k, v in nemenyi_results.items() if v['significant']]\n",
    "        \n",
    "        if significant_pairs:\n",
    "            for pair_name, result in significant_pairs:\n",
    "                print(f\"  {pair_name.replace('_vs_', ' > ')}: {result['better_algorithm']} significantly better\")\n",
    "        else:\n",
    "            print(\"  No significant pairwise differences found\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\nFriedman test not significant (p = {friedman_p:.6f})\")\n",
    "        print(\"Skipping post-hoc analysis - no significant differences between algorithms\")\n",
    "    \n",
    "    return {\n",
    "        'friedman_statistic': friedman_stat,\n",
    "        'friedman_p_value': friedman_p,\n",
    "        'mean_ranks': dict(zip(algorithms, mean_ranks)),\n",
    "        'performance_matrix': df_performance,\n",
    "        'ranks_matrix': df_ranks,\n",
    "        'nemenyi_results': nemenyi_results,\n",
    "        'critical_difference': critical_difference,\n",
    "        'significant_pairs': len([v for v in (nemenyi_results or {}).values() if v['significant']])\n",
    "    }\n",
    "\n",
    "def comprehensive_statistical_analysis(all_results):\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Analyze classification results\n",
    "    classification_results = friedman_nemenyi_test(all_results, 'classification')\n",
    "    results['classification'] = classification_results\n",
    "    \n",
    "    # Analyze regression results  \n",
    "    regression_results = friedman_nemenyi_test(all_results, 'regression')\n",
    "    results['regression'] = regression_results\n",
    "    \n",
    "    # Overall summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"OVERALL SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for problem_type, analysis in results.items():\n",
    "        print(f\"\\n{problem_type.upper()}:\")\n",
    "        print(f\"  Friedman test p-value: {analysis['friedman_p_value']:.6f}\")\n",
    "        print(f\"  Significant overall difference: {'Yes' if analysis['friedman_p_value'] < 0.05 else 'No'}\")\n",
    "        \n",
    "        if analysis['nemenyi_results']:\n",
    "            print(f\"  Significant pairwise differences: {analysis['significant_pairs']}\")\n",
    "            \n",
    "            # Rank the algorithms\n",
    "            mean_ranks = analysis['mean_ranks']\n",
    "            sorted_algorithms = sorted(mean_ranks.items(), key=lambda x: x[1])\n",
    "            print(f\"  Algorithm ranking (best to worst):\")\n",
    "            for rank, (alg, mean_rank) in enumerate(sorted_algorithms, 1):\n",
    "                print(f\"    {rank}. {alg} (mean rank: {mean_rank:.3f})\")\n",
    "        else:\n",
    "            print(f\"  Post-hoc analysis: Not performed (Friedman test not significant)\")\n",
    "    \n",
    "    return results\n",
    "statistical_results = comprehensive_statistical_analysis(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e66e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wins_ties_losses_vs_baseline(all_results, baseline='passive'):\n",
    "    # Active learning methods to compare against baseline\n",
    "    active_methods = ['uncertainty', 'sasla']\n",
    "    \n",
    "    # Performance measures for each problem type\n",
    "    classification_measures = ['f1_score', 'accuracy', 'precision', 'recall']\n",
    "    regression_measures = ['r2_score', 'mae', 'mse', 'root_mse']\n",
    "    \n",
    "    # Initialize results tracking\n",
    "    comparison_results = {method: {'wins': 0, 'ties': 0, 'losses': 0, 'details': []} \n",
    "                         for method in active_methods}\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"WINS/TIES/LOSSES ANALYSIS AGAINST BASELINE ({baseline.upper()})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # For each problem type\n",
    "    for problem_type in ['classification', 'regression']:\n",
    "        datasets = list(all_results[problem_type].keys())\n",
    "        measures = classification_measures if problem_type == 'classification' else regression_measures\n",
    "        \n",
    "        print(f\"\\n{problem_type.upper()} DATASETS:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for dataset in datasets:\n",
    "            print(f\"\\nAnalyzing {dataset}...\")\n",
    "            \n",
    "            # Get baseline results\n",
    "            baseline_results = all_results[problem_type][dataset][baseline]\n",
    "            if not baseline_results:\n",
    "                print(f\"  Warning: No baseline results for {dataset}\")\n",
    "                continue\n",
    "            \n",
    "            # For each performance measure\n",
    "            for measure in measures:\n",
    "                print(f\"  Measure: {measure}\")\n",
    "                \n",
    "                # Check if measure exists in baseline\n",
    "                if measure not in baseline_results[0]:\n",
    "                    print(f\"    Warning: {measure} not found in baseline results\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract baseline performance values\n",
    "                baseline_values = [run[measure] for run in baseline_results]\n",
    "                \n",
    "                # Compare each active method against baseline\n",
    "                for method in active_methods:\n",
    "                    method_results = all_results[problem_type][dataset][method]\n",
    "                    \n",
    "                    if not method_results:\n",
    "                        print(f\"    Warning: No results for {method}\")\n",
    "                        continue\n",
    "                    \n",
    "                    if measure not in method_results[0]:\n",
    "                        print(f\"    Warning: {measure} not found for {method}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract method performance values\n",
    "                    method_values = [run[measure] for run in method_results]\n",
    "                    \n",
    "                    # Ensure we have the same number of runs\n",
    "                    min_runs = min(len(baseline_values), len(method_values))\n",
    "                    if min_runs < len(baseline_values) or min_runs < len(method_values):\n",
    "                        print(f\"    Warning: Mismatched run counts for {method} vs {baseline}\")\n",
    "                        baseline_values_paired = baseline_values[:min_runs]\n",
    "                        method_values_paired = method_values[:min_runs]\n",
    "                    else:\n",
    "                        baseline_values_paired = baseline_values\n",
    "                        method_values_paired = method_values\n",
    "                    \n",
    "                    # Perform paired t-test\n",
    "                    try:\n",
    "                        # Determine test direction based on measure type\n",
    "                        higher_is_better = measure in ['f1_score', 'accuracy', 'precision', 'recall', 'r2_score']\n",
    "                        \n",
    "                        # Calculate differences (method - baseline)\n",
    "                        differences = np.array(method_values_paired) - np.array(baseline_values_paired)\n",
    "                        \n",
    "                        # Perform one-tailed t-test\n",
    "                        if higher_is_better:\n",
    "                            # For measures where higher is better, test if method > baseline\n",
    "                            t_stat, p_val = ttest_rel(method_values_paired, baseline_values_paired, \n",
    "                                                    alternative='greater')\n",
    "                            comparison_direction = \"higher is better\"\n",
    "                        else:\n",
    "                            # For measures where lower is better, test if method < baseline\n",
    "                            t_stat, p_val = ttest_rel(method_values_paired, baseline_values_paired, \n",
    "                                                    alternative='less')\n",
    "                            comparison_direction = \"lower is better\"\n",
    "                        \n",
    "                        # Determine result\n",
    "                        alpha = 0.05\n",
    "                        if p_val < alpha:\n",
    "                            result = 'Win'\n",
    "                            comparison_results[method]['wins'] += 1\n",
    "                        else:\n",
    "                            # Check if significantly worse (opposite direction)\n",
    "                            if higher_is_better:\n",
    "                                t_stat_worse, p_val_worse = ttest_rel(method_values_paired, baseline_values_paired, \n",
    "                                                                    alternative='less')\n",
    "                            else:\n",
    "                                t_stat_worse, p_val_worse = ttest_rel(method_values_paired, baseline_values_paired, \n",
    "                                                                    alternative='greater')\n",
    "                            \n",
    "                            if p_val_worse < alpha:\n",
    "                                result = 'Loss'\n",
    "                                comparison_results[method]['losses'] += 1\n",
    "                            else:\n",
    "                                result = 'Tie'\n",
    "                                comparison_results[method]['ties'] += 1\n",
    "                        \n",
    "                        # Calculate effect size (mean difference)\n",
    "                        mean_baseline = np.mean(baseline_values_paired)\n",
    "                        mean_method = np.mean(method_values_paired)\n",
    "                        effect_size = mean_method - mean_baseline\n",
    "                        \n",
    "                        print(f\"    {method} vs {baseline}:\")\n",
    "                        print(f\"      {baseline}: {mean_baseline:.4f}\")\n",
    "                        print(f\"      {method}: {mean_method:.4f}\")\n",
    "                        print(f\"      Effect size: {effect_size:+.4f}\")\n",
    "                        print(f\"      t-statistic: {t_stat:.4f}\")\n",
    "                        print(f\"      p-value: {p_val:.6f}\")\n",
    "                        print(f\"      Result: {result}\")\n",
    "                        \n",
    "                        # Store detailed results\n",
    "                        comparison_results[method]['details'].append({\n",
    "                            'dataset': dataset,\n",
    "                            'problem_type': problem_type,\n",
    "                            'measure': measure,\n",
    "                            'result': result,\n",
    "                            'p_value': p_val,\n",
    "                            'effect_size': effect_size,\n",
    "                            'baseline_mean': mean_baseline,\n",
    "                            'method_mean': mean_method,\n",
    "                            'comparison_direction': comparison_direction\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"    Error in t-test for {method} vs {baseline}: {e}\")\n",
    "                        comparison_results[method]['ties'] += 1  # Default to tie on error\n",
    "    \n",
    "    return comparison_results\n",
    "\n",
    "def create_baseline_comparison_summary(comparison_results, baseline='passive'):\n",
    "    \"\"\"\n",
    "    Create a comprehensive summary of wins/ties/losses against baseline.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SUMMARY: ACTIVE LEARNING vs {baseline.upper()} BASELINE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Overall summary table\n",
    "    print(f\"\\nOVERALL COMPARISON SUMMARY\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Method':<15} {'Wins':<10} {'Ties':<10} {'Losses':<10} {'Win Rate':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for method in ['uncertainty', 'sasla']:\n",
    "        wins = comparison_results[method]['wins']\n",
    "        ties = comparison_results[method]['ties']\n",
    "        losses = comparison_results[method]['losses']\n",
    "        total_comparisons = wins + ties + losses\n",
    "        win_rate = (wins / total_comparisons) * 100 if total_comparisons > 0 else 0\n",
    "        \n",
    "        print(f\"{method:<15} {wins:<10} {ties:<10} {losses:<10} {win_rate:<10.1f}%\")\n",
    "    \n",
    "    # Problem-specific breakdown\n",
    "    problem_summary = {\n",
    "        'classification': {method: {'wins': 0, 'ties': 0, 'losses': 0} \n",
    "                          for method in ['uncertainty', 'sasla']},\n",
    "        'regression': {method: {'wins': 0, 'ties': 0, 'losses': 0} \n",
    "                      for method in ['uncertainty', 'sasla']}\n",
    "    }\n",
    "    \n",
    "    # Aggregate by problem type\n",
    "    for method in ['uncertainty', 'sasla']:\n",
    "        for detail in comparison_results[method]['details']:\n",
    "            problem_type = detail['problem_type']\n",
    "            result = detail['result']\n",
    "            \n",
    "            if result == 'Win':\n",
    "                problem_summary[problem_type][method]['wins'] += 1\n",
    "            elif result == 'Loss':\n",
    "                problem_summary[problem_type][method]['losses'] += 1\n",
    "            else:\n",
    "                problem_summary[problem_type][method]['ties'] += 1\n",
    "    \n",
    "    # Display by problem type\n",
    "    for problem_type in ['classification', 'regression']:\n",
    "        print(f\"\\n{problem_type.upper()} RESULTS\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Method':<15} {'Wins':<10} {'Ties':<10} {'Losses':<10} {'Win Rate':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for method in ['uncertainty', 'sasla']:\n",
    "            wins = problem_summary[problem_type][method]['wins']\n",
    "            ties = problem_summary[problem_type][method]['ties']\n",
    "            losses = problem_summary[problem_type][method]['losses']\n",
    "            total = wins + ties + losses\n",
    "            win_rate = (wins / total) * 100 if total > 0 else 0\n",
    "            print(f\"{method:<15} {wins:<10} {ties:<10} {losses:<10} {win_rate:<10.1f}%\")\n",
    "    \n",
    "    # Detailed breakdown by dataset and measure\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"DETAILED BREAKDOWN BY DATASET AND MEASURE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for method in ['uncertainty', 'sasla']:\n",
    "        print(f\"\\n{method.upper()} vs {baseline.upper()}:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Group by dataset\n",
    "        dataset_results = {}\n",
    "        for detail in comparison_results[method]['details']:\n",
    "            dataset = detail['dataset']\n",
    "            if dataset not in dataset_results:\n",
    "                dataset_results[dataset] = []\n",
    "            dataset_results[dataset].append(detail)\n",
    "        \n",
    "        for dataset, results in dataset_results.items():\n",
    "            print(f\"\\n  {dataset}:\")\n",
    "            wins = len([r for r in results if r['result'] == 'Win'])\n",
    "            ties = len([r for r in results if r['result'] == 'Tie'])\n",
    "            losses = len([r for r in results if r['result'] == 'Loss'])\n",
    "            print(f\"    Overall: {wins}W/{ties}T/{losses}L\")\n",
    "            \n",
    "            for result in results:\n",
    "                result_symbol = 'W' if result['result'] == 'Win' else ('L' if result['result'] == 'Loss' else 'T')\n",
    "                print(f\"    {result['measure']:<12}: {result_symbol} \"\n",
    "                      f\"(p={result['p_value']:.4f}, effect={result['effect_size']:+.4f})\")\n",
    "    \n",
    "    # Statistical significance summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STATISTICAL SIGNIFICANCE SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for method in ['uncertainty', 'sasla']:\n",
    "        significant_wins = len([d for d in comparison_results[method]['details'] \n",
    "                              if d['result'] == 'Win'])\n",
    "        significant_losses = len([d for d in comparison_results[method]['details'] \n",
    "                                if d['result'] == 'Loss'])\n",
    "        total_comparisons = len(comparison_results[method]['details'])\n",
    "        \n",
    "        print(f\"\\n{method.upper()}:\")\n",
    "        print(f\"  Significantly better than {baseline}: {significant_wins}/{total_comparisons}\")\n",
    "        print(f\"  Significantly worse than {baseline}: {significant_losses}/{total_comparisons}\")\n",
    "        print(f\"  No significant difference: {total_comparisons - significant_wins - significant_losses}/{total_comparisons}\")\n",
    "    \n",
    "    return problem_summary\n",
    "\n",
    "all_results = load_all_results()\n",
    "baseline_comparison = calculate_wins_ties_losses_vs_baseline(all_results, baseline='passive')\n",
    "baseline_summary = create_baseline_comparison_summary(baseline_comparison, baseline='passive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00aa68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10150f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
